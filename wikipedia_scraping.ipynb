{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import tiktoken\n",
    "from urllib.parse import unquote\n",
    "from urllib.request import urlopen\n",
    "# Cell to import necessary libraries\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_runnable,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Updated tool for Scrapping ############################\n",
    "\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "def parse_page(html, base_url):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Extract the title for the directory name\n",
    "    title_tag = soup.find('h1')\n",
    "    title_text = title_tag.text.replace('/', '_')  # Replace '/' in titles with '_' to avoid path issues\n",
    "    \n",
    "    # Extract all text from the main content\n",
    "    content = soup.find('div', {'id': 'mw-content-text'})\n",
    "    text = content.get_text() if content else \"\"\n",
    "    \n",
    "    # Find links within the content that include the main title\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True) if a.text and title_text in a.text]\n",
    "    \n",
    "    # Convert relative links to absolute\n",
    "    links = ['https://en.wikipedia.org' + link if link.startswith('/wiki/') else link for link in links]\n",
    "    \n",
    "    return text, links, title_text\n",
    "    \n",
    "\n",
    "def scrape_wikipedia(start_url):\n",
    "    html = fetch_page(start_url)\n",
    "    text, links, title = parse_page(html, start_url)\n",
    "    \n",
    "    # For simplicity, just fetch text from the first few relevant links\n",
    "    for link in links[:10]:  # Limit to first 10 links to avoid too many requests\n",
    "        html = fetch_page(link)\n",
    "        page_text, _ , _ = parse_page(html, link)  # We don't follow further links here\n",
    "        text += \"\\n\\n\" + page_text\n",
    "\n",
    "    base_dir = os.path.join('data/scraped_data', title)\n",
    "    os.makedirs(base_dir, exist_ok=True)  # Create directory if it does not exist\n",
    "    \n",
    "    # Format current datetime for the filename\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = f\"{title}_{now}_Wikipedia.txt\"\n",
    "    filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    # Save the content to a text file\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "    return text, title\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "url = 'https://en.wikipedia.org/wiki/Interstellar_(film)'\n",
    "collected_text, title = scrape_wikipedia(url)\n",
    "print(len(collected_text))\n",
    "print(f\"Data collected for {title} and saved to the respective directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################Scrape further links for Fetched Entities of Chatacters like acotes, director etc ##################\n",
    "\n",
    "def read_all_json_files(directory_path):\n",
    "    \"\"\"Read all JSON files in the given directory and extract unique character names and aliases.\"\"\"\n",
    "    names = set()\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            for item in data:\n",
    "                names.add(item['character'])\n",
    "                names.update(item.get('aliases', []))\n",
    "    return names\n",
    "\n",
    "def find_relevant_links(soup, names):\n",
    "    \"\"\"Find and return links that are relevant based on the names provided.\"\"\"\n",
    "    relevant_links = []\n",
    "    for link in soup.find_all('a', href=True, title=True):\n",
    "        if any(name in link['title'] for name in names):\n",
    "            relevant_links.append(link['href'])\n",
    "    return relevant_links\n",
    "\n",
    "def scrape_relevant_content(start_url, root_json_dir):\n",
    "    \"\"\"Scrape content from Wikipedia based on character names extracted from JSON files in a specific directory.\"\"\"\n",
    "    title = unquote(start_url.split('/')[-1]).replace('_', ' ')\n",
    "    json_dir_path = os.path.join(root_json_dir, title)\n",
    "    \n",
    "    if not os.path.exists(json_dir_path):\n",
    "        print(f\"No JSON directory found for title: {title}\")\n",
    "        return\n",
    "    \n",
    "    names = read_all_json_files(json_dir_path)\n",
    "    response = requests.get(start_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    relevant_links = find_relevant_links(soup, names)\n",
    "    base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "    for href in relevant_links:\n",
    "        full_url = base_url + href\n",
    "        response = requests.get(full_url)\n",
    "        page_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_title = unquote(href.split('/')[-1]).replace('_', ' ')\n",
    "        dir_path = os.path.join('data/targeted_scraped_data', page_title)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        file_path = os.path.join(dir_path, f\"{page_title}.txt\")\n",
    "\n",
    "        # Extract and save only the textual content from the article\n",
    "        text_content = page_soup.find('div', id='mw-content-text').get_text(separator='\\n', strip=True)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text_content)\n",
    "        print(f\"Saved content related to {page_title} to {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "scrape_relevant_content('https://en.wikipedia.org/wiki/Interstellar_(film)', 'data/Entity_json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################Scrape further links present in the provided wiki Page (may stuck into recurssion loop) ##################\n",
    "\n",
    "def scrape_wikipedia_content(url, depth=0, max_depth=1, iteration=1, max_iterations=20, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()  # Initialize the set of visited URLs\n",
    "    \n",
    "    if depth > max_depth or iteration > max_iterations:\n",
    "        print(f\"Stopping recursion at depth {depth} and iteration {iteration}\")\n",
    "        return  # Stop recursion based on depth and iteration limits\n",
    "\n",
    "    if url in visited:\n",
    "        print(f\"Already visited {url}\")\n",
    "        return  # Avoid re-scraping the same URL\n",
    "    visited.add(url)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.find(id='firstHeading').text.replace('/', '_')  # Sanitize title for file path\n",
    "    dir_path = os.path.join('data/rough_scraped_data', title)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    content = soup.find(id='mw-content-text')\n",
    "    if content:\n",
    "        content_text = content.get_text()\n",
    "        file_path = os.path.join(dir_path, f\"{title}_Wikipedia_chunk{iteration}.txt\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(content_text)\n",
    "        print(f\"Saved content to {file_path}\")\n",
    "\n",
    "    # Recurse only if under max iterations\n",
    "    if iteration < max_iterations:\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and not ':' in href:\n",
    "                full_url = f'https://en.wikipedia.org{href}'\n",
    "                if full_url not in visited:\n",
    "                    print(f\"Recursing into {full_url} at iteration {iteration+1}\")\n",
    "                    scrape_wikipedia_content(full_url, depth + 1, max_depth, iteration + 1, max_iterations, visited)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "scrape_wikipedia_content('https://en.wikipedia.org/wiki/Interstellar_(film)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### For scraping using wikipedia api but currently not in use (problem not fetch all data) ############################\n",
    "# Main function to scrape and save data\n",
    "\n",
    "# General function to scrape data from any URL\n",
    "def scrape_general(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    data = soup.get_text()\n",
    "    return data\n",
    "\n",
    "# Function to create directories based on title and source\n",
    "def create_directory_structure(base_dir, title, source):\n",
    "    dir_path = os.path.join(base_dir, title, source)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n",
    "\n",
    "\n",
    "# Function to save data to a file with structured naming\n",
    "def save_data(data, dir_path, title, source):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    file_name = f\"{title}_{source}_{timestamp}.txt\"\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(data)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def scrape_and_save(title, url=None):\n",
    "    base_dir = \"scraped_data\"\n",
    "    if url:\n",
    "        source = url.split(\"//\")[-1].split(\"/\")[0]\n",
    "        data = scrape_general(url)\n",
    "    else:\n",
    "        source = \"wikipedia\"\n",
    "        data = scrape_wikipedia(title)\n",
    "    \n",
    "    dir_path = create_directory_structure(base_dir, title, source)\n",
    "    file_path = save_data(data, dir_path, title, source)\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "\n",
    "title = \"Interstellar_(film)\"\n",
    "wikipedia_url = \"https://en.wikipedia.org/wiki/Interstellar_(film)\"\n",
    "# fandom_url = \"https://interstellarfilm.fandom.com/wiki/Interstellar_Wiki\"\n",
    "\n",
    "# Scrape and save data from Wikipedia\n",
    "wiki_file_path = scrape_and_save(title)\n",
    "print(f\"Data saved to: {wiki_file_path}\")\n",
    "\n",
    "# # Scrape and save data from Fandom\n",
    "# fandom_file_path = scrape_and_save(title, fandom_url)\n",
    "# print(f\"Data saved to: {fandom_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: requests in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (4.12.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from beautifulsoup4) (2.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/ATIFHANIF/.pyenv/versions/3.7.12/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: python-dotenv in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (0.21.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/ATIFHANIF/.pyenv/versions/3.7.12/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-openai (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for langchain-openai\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/ATIFHANIF/.pyenv/versions/3.7.12/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting openai\n",
      "  Using cached openai-1.33.0-py3-none-any.whl (325 kB)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Using cached pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "Collecting cached-property; python_version < \"3.8\"\n",
      "  Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied, skipping upgrade: anyio<5,>=3.5.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied, skipping upgrade: sniffio in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions<5,>=4.7 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (4.7.1)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version == \"3.7\" in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from pydantic<3,>=1.9.0->openai) (6.7.0)\n",
      "Collecting pydantic-core==2.14.6\n",
      "  Using cached pydantic_core-2.14.6.tar.gz (360 kB)\n",
      "  Installing build dependencies ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Users/ATIFHANIF/.pyenv/versions/3.7.12/bin/python /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages/pip install --ignore-installed --no-user --prefix /private/var/folders/lj/knh84kr15sdflszx47jnzy480000gp/T/pip-build-env-niheb3hz/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'maturin>=1,<2' 'typing-extensions >=4.6.0,!=4.7.0'\n",
      "       cwd: None\n",
      "  Complete output (58 lines):\n",
      "  Collecting maturin<2,>=1\n",
      "    Using cached maturin-1.6.0.tar.gz (187 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'done'\n",
      "    Getting requirements to build wheel: started\n",
      "    Getting requirements to build wheel: finished with status 'done'\n",
      "      Preparing wheel metadata: started\n",
      "      Preparing wheel metadata: finished with status 'done'\n",
      "  Collecting typing-extensions!=4.7.0,>=4.6.0\n",
      "    Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "  Collecting tomli>=1.1.0; python_version < \"3.11\"\n",
      "    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "  Building wheels for collected packages: maturin\n",
      "    Building wheel for maturin (PEP 517): started\n",
      "    Building wheel for maturin (PEP 517): finished with status 'error'\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/ATIFHANIF/.pyenv/versions/3.7.12/bin/python /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /var/folders/lj/knh84kr15sdflszx47jnzy480000gp/T/tmpo9zlqfvv\n",
      "         cwd: /private/var/folders/lj/knh84kr15sdflszx47jnzy480000gp/T/pip-install-pak7wpp2/maturin\n",
      "    Complete output (33 lines):\n",
      "    running bdist_wheel\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.macosx-14.0-arm64-cpython-37\n",
      "    creating build/lib.macosx-14.0-arm64-cpython-37/maturin\n",
      "    copying maturin/__init__.py -> build/lib.macosx-14.0-arm64-cpython-37/maturin\n",
      "    copying maturin/import_hook.py -> build/lib.macosx-14.0-arm64-cpython-37/maturin\n",
      "    copying maturin/__main__.py -> build/lib.macosx-14.0-arm64-cpython-37/maturin\n",
      "    running egg_info\n",
      "    creating maturin.egg-info\n",
      "    writing maturin.egg-info/PKG-INFO\n",
      "    writing dependency_links to maturin.egg-info/dependency_links.txt\n",
      "    writing requirements to maturin.egg-info/requires.txt\n",
      "    writing top-level names to maturin.egg-info/top_level.txt\n",
      "    writing manifest file 'maturin.egg-info/SOURCES.txt'\n",
      "    reading manifest file 'maturin.egg-info/SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    warning: no files found matching '*.json' under directory 'src/python_interpreter'\n",
      "    writing manifest file 'maturin.egg-info/SOURCES.txt'\n",
      "    running build_ext\n",
      "    running build_rust\n",
      "    error: can't find Rust compiler\n",
      "  \n",
      "    If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "    To update pip, run:\n",
      "  \n",
      "        pip install --upgrade pip\n",
      "  \n",
      "    and then retry package installation.\n",
      "  \n",
      "    If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "    ----------------------------------------\n",
      "    ERROR: Failed building wheel for maturin\n",
      "  Failed to build maturin\n",
      "  ERROR: Could not build wheels for maturin which use PEP 517 and cannot be installed directly\n",
      "  WARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "  You should consider upgrading via the '/Users/ATIFHANIF/.pyenv/versions/3.7.12/bin/python -m pip install --upgrade pip' command.\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Command errored out with exit status 1: /Users/ATIFHANIF/.pyenv/versions/3.7.12/bin/python /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages/pip install --ignore-installed --no-user --prefix /private/var/folders/lj/knh84kr15sdflszx47jnzy480000gp/T/pip-build-env-niheb3hz/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'maturin>=1,<2' 'typing-extensions >=4.6.0,!=4.7.0' Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/ATIFHANIF/.pyenv/versions/3.7.12/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Cell to install necessary packages\n",
    "! pip install wikipedia-api requests beautifulsoup4\n",
    "%pip install python-dotenv\n",
    "%pip install --quiet langchain openai langchain-openai unstructured\n",
    "%pip install --upgrade openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (24.0)\n",
      "Requirement already satisfied: openai in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (1.33.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: cached-property in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (1.5.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (0.24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from httpx<1,>=0.23.0->openai) (0.17.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from pydantic<3,>=1.9.0->openai) (0.5.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from pydantic<3,>=1.9.0->openai) (6.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from importlib-metadata->pydantic<3,>=1.9.0->openai) (3.15.0)\n",
      "Requirement already satisfied: nltk in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: importlib-metadata in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from click->nltk) (6.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.7.1)\n",
      "Requirement already satisfied: wikipedia-api in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: requests in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (0.21.1)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.0.1 Requires-Python >=3.8.1,<4.0; 0.0.100 Requires-Python >=3.8.1,<4.0; 0.0.101 Requires-Python >=3.8.1,<4.0; 0.0.101rc0 Requires-Python >=3.8.1,<4.0; 0.0.102 Requires-Python >=3.8.1,<4.0; 0.0.102rc0 Requires-Python >=3.8.1,<4.0; 0.0.103 Requires-Python >=3.8.1,<4.0; 0.0.104 Requires-Python >=3.8.1,<4.0; 0.0.105 Requires-Python >=3.8.1,<4.0; 0.0.106 Requires-Python >=3.8.1,<4.0; 0.0.107 Requires-Python >=3.8.1,<4.0; 0.0.108 Requires-Python >=3.8.1,<4.0; 0.0.109 Requires-Python >=3.8.1,<4.0; 0.0.110 Requires-Python >=3.8.1,<4.0; 0.0.111 Requires-Python >=3.8.1,<4.0; 0.0.112 Requires-Python >=3.8.1,<4.0; 0.0.113 Requires-Python >=3.8.1,<4.0; 0.0.114 Requires-Python >=3.8.1,<4.0; 0.0.115 Requires-Python >=3.8.1,<4.0; 0.0.116 Requires-Python >=3.8.1,<4.0; 0.0.117 Requires-Python >=3.8.1,<4.0; 0.0.118 Requires-Python >=3.8.1,<4.0; 0.0.119 Requires-Python >=3.8.1,<4.0; 0.0.120 Requires-Python >=3.8.1,<4.0; 0.0.121 Requires-Python >=3.8.1,<4.0; 0.0.122 Requires-Python >=3.8.1,<4.0; 0.0.123 Requires-Python >=3.8.1,<4.0; 0.0.124 Requires-Python >=3.8.1,<4.0; 0.0.125 Requires-Python >=3.8.1,<4.0; 0.0.126 Requires-Python >=3.8.1,<4.0; 0.0.127 Requires-Python >=3.8.1,<4.0; 0.0.128 Requires-Python >=3.8.1,<4.0; 0.0.129 Requires-Python >=3.8.1,<4.0; 0.0.130 Requires-Python >=3.8.1,<4.0; 0.0.131 Requires-Python >=3.8.1,<4.0; 0.0.132 Requires-Python >=3.8.1,<4.0; 0.0.133 Requires-Python >=3.8.1,<4.0; 0.0.134 Requires-Python >=3.8.1,<4.0; 0.0.135 Requires-Python >=3.8.1,<4.0; 0.0.136 Requires-Python >=3.8.1,<4.0; 0.0.137 Requires-Python >=3.8.1,<4.0; 0.0.138 Requires-Python >=3.8.1,<4.0; 0.0.139 Requires-Python >=3.8.1,<4.0; 0.0.140 Requires-Python >=3.8.1,<4.0; 0.0.141 Requires-Python >=3.8.1,<4.0; 0.0.142 Requires-Python >=3.8.1,<4.0; 0.0.143 Requires-Python >=3.8.1,<4.0; 0.0.144 Requires-Python >=3.8.1,<4.0; 0.0.145 Requires-Python >=3.8.1,<4.0; 0.0.146 Requires-Python >=3.8.1,<4.0; 0.0.147 Requires-Python >=3.8.1,<4.0; 0.0.148 Requires-Python >=3.8.1,<4.0; 0.0.149 Requires-Python >=3.8.1,<4.0; 0.0.150 Requires-Python >=3.8.1,<4.0; 0.0.151 Requires-Python >=3.8.1,<4.0; 0.0.152 Requires-Python >=3.8.1,<4.0; 0.0.153 Requires-Python >=3.8.1,<4.0; 0.0.154 Requires-Python >=3.8.1,<4.0; 0.0.155 Requires-Python >=3.8.1,<4.0; 0.0.156 Requires-Python >=3.8.1,<4.0; 0.0.157 Requires-Python >=3.8.1,<4.0; 0.0.158 Requires-Python >=3.8.1,<4.0; 0.0.159 Requires-Python >=3.8.1,<4.0; 0.0.160 Requires-Python >=3.8.1,<4.0; 0.0.161 Requires-Python >=3.8.1,<4.0; 0.0.162 Requires-Python >=3.8.1,<4.0; 0.0.163 Requires-Python >=3.8.1,<4.0; 0.0.164 Requires-Python >=3.8.1,<4.0; 0.0.165 Requires-Python >=3.8.1,<4.0; 0.0.166 Requires-Python >=3.8.1,<4.0; 0.0.167 Requires-Python >=3.8.1,<4.0; 0.0.168 Requires-Python >=3.8.1,<4.0; 0.0.169 Requires-Python >=3.8.1,<4.0; 0.0.170 Requires-Python >=3.8.1,<4.0; 0.0.171 Requires-Python >=3.8.1,<4.0; 0.0.172 Requires-Python >=3.8.1,<4.0; 0.0.173 Requires-Python >=3.8.1,<4.0; 0.0.174 Requires-Python >=3.8.1,<4.0; 0.0.175 Requires-Python >=3.8.1,<4.0; 0.0.176 Requires-Python >=3.8.1,<4.0; 0.0.177 Requires-Python >=3.8.1,<4.0; 0.0.178 Requires-Python >=3.8.1,<4.0; 0.0.179 Requires-Python >=3.8.1,<4.0; 0.0.180 Requires-Python >=3.8.1,<4.0; 0.0.181 Requires-Python >=3.8.1,<4.0; 0.0.182 Requires-Python >=3.8.1,<4.0; 0.0.183 Requires-Python >=3.8.1,<4.0; 0.0.184 Requires-Python >=3.8.1,<4.0; 0.0.185 Requires-Python >=3.8.1,<4.0; 0.0.186 Requires-Python >=3.8.1,<4.0; 0.0.187 Requires-Python >=3.8.1,<4.0; 0.0.188 Requires-Python >=3.8.1,<4.0; 0.0.189 Requires-Python >=3.8.1,<4.0; 0.0.190 Requires-Python >=3.8.1,<4.0; 0.0.191 Requires-Python >=3.8.1,<4.0; 0.0.192 Requires-Python >=3.8.1,<4.0; 0.0.193 Requires-Python >=3.8.1,<4.0; 0.0.194 Requires-Python >=3.8.1,<4.0; 0.0.195 Requires-Python >=3.8.1,<4.0; 0.0.196 Requires-Python >=3.8.1,<4.0; 0.0.197 Requires-Python >=3.8.1,<4.0; 0.0.198 Requires-Python >=3.8.1,<4.0; 0.0.199 Requires-Python >=3.8.1,<4.0; 0.0.1rc0 Requires-Python >=3.8.1,<4.0; 0.0.1rc1 Requires-Python >=3.8.1,<4.0; 0.0.2 Requires-Python >=3.8.1,<4.0; 0.0.2.post1 Requires-Python >=3.8.1,<4.0; 0.0.200 Requires-Python >=3.8.1,<4.0; 0.0.201 Requires-Python >=3.8.1,<4.0; 0.0.202 Requires-Python >=3.8.1,<4.0; 0.0.203 Requires-Python >=3.8.1,<4.0; 0.0.204 Requires-Python >=3.8.1,<4.0; 0.0.205 Requires-Python >=3.8.1,<4.0; 0.0.206 Requires-Python >=3.8.1,<4.0; 0.0.207 Requires-Python >=3.8.1,<4.0; 0.0.208 Requires-Python >=3.8.1,<4.0; 0.0.209 Requires-Python >=3.8.1,<4.0; 0.0.210 Requires-Python >=3.8.1,<4.0; 0.0.211 Requires-Python >=3.8.1,<4.0; 0.0.212 Requires-Python >=3.8.1,<4.0; 0.0.213 Requires-Python >=3.8.1,<4.0; 0.0.214 Requires-Python >=3.8.1,<4.0; 0.0.215 Requires-Python >=3.8.1,<4.0; 0.0.216 Requires-Python >=3.8.1,<4.0; 0.0.217 Requires-Python >=3.8.1,<4.0; 0.0.218 Requires-Python >=3.8.1,<4.0; 0.0.219 Requires-Python >=3.8.1,<4.0; 0.0.220 Requires-Python >=3.8.1,<4.0; 0.0.221 Requires-Python >=3.8.1,<4.0; 0.0.222 Requires-Python >=3.8.1,<4.0; 0.0.223 Requires-Python >=3.8.1,<4.0; 0.0.224 Requires-Python >=3.8.1,<4.0; 0.0.225 Requires-Python >=3.8.1,<4.0; 0.0.226 Requires-Python >=3.8.1,<4.0; 0.0.227 Requires-Python >=3.8.1,<4.0; 0.0.228 Requires-Python >=3.8.1,<4.0; 0.0.229 Requires-Python >=3.8.1,<4.0; 0.0.230 Requires-Python >=3.8.1,<4.0; 0.0.231 Requires-Python >=3.8.1,<4.0; 0.0.232 Requires-Python >=3.8.1,<4.0; 0.0.233 Requires-Python >=3.8.1,<4.0; 0.0.234 Requires-Python >=3.8.1,<4.0; 0.0.235 Requires-Python >=3.8.1,<4.0; 0.0.236 Requires-Python >=3.8.1,<4.0; 0.0.237 Requires-Python >=3.8.1,<4.0; 0.0.238 Requires-Python >=3.8.1,<4.0; 0.0.239 Requires-Python >=3.8.1,<4.0; 0.0.240 Requires-Python >=3.8.1,<4.0; 0.0.240rc0 Requires-Python >=3.8.1,<4.0; 0.0.240rc1 Requires-Python >=3.8.1,<4.0; 0.0.240rc4 Requires-Python >=3.8.1,<4.0; 0.0.242 Requires-Python >=3.8.1,<4.0; 0.0.243 Requires-Python >=3.8.1,<4.0; 0.0.244 Requires-Python >=3.8.1,<4.0; 0.0.245 Requires-Python >=3.8.1,<4.0; 0.0.246 Requires-Python >=3.8.1,<4.0; 0.0.247 Requires-Python >=3.8.1,<4.0; 0.0.248 Requires-Python >=3.8.1,<4.0; 0.0.249 Requires-Python >=3.8.1,<4.0; 0.0.250 Requires-Python >=3.8.1,<4.0; 0.0.251 Requires-Python >=3.8.1,<4.0; 0.0.252 Requires-Python >=3.8.1,<4.0; 0.0.253 Requires-Python >=3.8.1,<4.0; 0.0.254 Requires-Python >=3.8.1,<4.0; 0.0.255 Requires-Python >=3.8.1,<4.0; 0.0.256 Requires-Python >=3.8.1,<4.0; 0.0.257 Requires-Python >=3.8.1,<4.0; 0.0.258 Requires-Python >=3.8.1,<4.0; 0.0.259 Requires-Python >=3.8.1,<4.0; 0.0.260 Requires-Python >=3.8.1,<4.0; 0.0.261 Requires-Python >=3.8.1,<4.0; 0.0.262 Requires-Python >=3.8.1,<4.0; 0.0.263 Requires-Python >=3.8.1,<4.0; 0.0.264 Requires-Python >=3.8.1,<4.0; 0.0.265 Requires-Python >=3.8.1,<4.0; 0.0.266 Requires-Python >=3.8.1,<4.0; 0.0.267 Requires-Python >=3.8.1,<4.0; 0.0.268 Requires-Python >=3.8.1,<4.0; 0.0.269 Requires-Python >=3.8.1,<4.0; 0.0.270 Requires-Python >=3.8.1,<4.0; 0.0.271 Requires-Python >=3.8.1,<4.0; 0.0.272 Requires-Python >=3.8.1,<4.0; 0.0.273 Requires-Python >=3.8.1,<4.0; 0.0.274 Requires-Python >=3.8.1,<4.0; 0.0.275 Requires-Python >=3.8.1,<4.0; 0.0.276 Requires-Python >=3.8.1,<4.0; 0.0.277 Requires-Python >=3.8.1,<4.0; 0.0.278 Requires-Python >=3.8.1,<4.0; 0.0.279 Requires-Python >=3.8.1,<4.0; 0.0.28 Requires-Python >=3.8.1,<4.0; 0.0.281 Requires-Python >=3.8.1,<4.0; 0.0.283 Requires-Python >=3.8.1,<4.0; 0.0.284 Requires-Python >=3.8.1,<4.0; 0.0.285 Requires-Python >=3.8.1,<4.0; 0.0.286 Requires-Python >=3.8.1,<4.0; 0.0.287 Requires-Python >=3.8.1,<4.0; 0.0.288 Requires-Python >=3.8.1,<4.0; 0.0.289 Requires-Python >=3.8.1,<4.0; 0.0.29 Requires-Python >=3.8.1,<4.0; 0.0.290 Requires-Python >=3.8.1,<4.0; 0.0.291 Requires-Python >=3.8.1,<4.0; 0.0.292 Requires-Python >=3.8.1,<4.0; 0.0.293 Requires-Python >=3.8.1,<4.0; 0.0.294 Requires-Python >=3.8.1,<4.0; 0.0.295 Requires-Python >=3.8.1,<4.0; 0.0.296 Requires-Python >=3.8.1,<4.0; 0.0.297 Requires-Python >=3.8.1,<4.0; 0.0.298 Requires-Python >=3.8.1,<4.0; 0.0.299 Requires-Python >=3.8.1,<4.0; 0.0.3 Requires-Python >=3.8.1,<4.0; 0.0.30 Requires-Python >=3.8.1,<4.0; 0.0.300 Requires-Python >=3.8.1,<4.0; 0.0.301 Requires-Python >=3.8.1,<4.0; 0.0.302 Requires-Python >=3.8.1,<4.0; 0.0.303 Requires-Python >=3.8.1,<4.0; 0.0.304 Requires-Python >=3.8.1,<4.0; 0.0.305 Requires-Python >=3.8.1,<4.0; 0.0.306 Requires-Python >=3.8.1,<4.0; 0.0.307 Requires-Python >=3.8.1,<4.0; 0.0.308 Requires-Python >=3.8.1,<4.0; 0.0.309 Requires-Python >=3.8.1,<4.0; 0.0.31 Requires-Python >=3.8.1,<4.0; 0.0.310 Requires-Python >=3.8.1,<4.0; 0.0.311 Requires-Python >=3.8.1,<4.0; 0.0.312 Requires-Python >=3.8.1,<4.0; 0.0.313 Requires-Python >=3.8.1,<4.0; 0.0.314 Requires-Python >=3.8.1,<4.0; 0.0.315 Requires-Python >=3.8.1,<4.0; 0.0.316 Requires-Python >=3.8.1,<4.0; 0.0.317 Requires-Python >=3.8.1,<4.0; 0.0.318 Requires-Python >=3.8.1,<4.0; 0.0.319 Requires-Python >=3.8.1,<4.0; 0.0.32 Requires-Python >=3.8.1,<4.0; 0.0.320 Requires-Python >=3.8.1,<4.0; 0.0.321 Requires-Python >=3.8.1,<4.0; 0.0.322 Requires-Python >=3.8.1,<4.0; 0.0.323 Requires-Python >=3.8.1,<4.0; 0.0.324 Requires-Python >=3.8.1,<4.0; 0.0.325 Requires-Python >=3.8.1,<4.0; 0.0.326 Requires-Python >=3.8.1,<4.0; 0.0.327 Requires-Python >=3.8.1,<4.0; 0.0.329 Requires-Python >=3.8.1,<4.0; 0.0.33 Requires-Python >=3.8.1,<4.0; 0.0.330 Requires-Python >=3.8.1,<4.0; 0.0.331 Requires-Python >=3.8.1,<4.0; 0.0.331rc0 Requires-Python >=3.8.1,<4.0; 0.0.331rc1 Requires-Python >=3.8.1,<4.0; 0.0.331rc2 Requires-Python >=3.8.1,<4.0; 0.0.331rc3 Requires-Python >=3.8.1,<4.0; 0.0.332 Requires-Python >=3.8.1,<4.0; 0.0.333 Requires-Python >=3.8.1,<4.0; 0.0.334 Requires-Python >=3.8.1,<4.0; 0.0.335 Requires-Python >=3.8.1,<4.0; 0.0.336 Requires-Python >=3.8.1,<4.0; 0.0.337 Requires-Python >=3.8.1,<4.0; 0.0.338 Requires-Python >=3.8.1,<4.0; 0.0.339 Requires-Python >=3.8.1,<4.0; 0.0.339rc0 Requires-Python >=3.8.1,<4.0; 0.0.339rc1 Requires-Python >=3.8.1,<4.0; 0.0.339rc2 Requires-Python >=3.8.1,<4.0; 0.0.339rc3 Requires-Python >=3.8.1,<4.0; 0.0.34 Requires-Python >=3.8.1,<4.0; 0.0.340 Requires-Python >=3.8.1,<4.0; 0.0.341 Requires-Python >=3.8.1,<4.0; 0.0.342 Requires-Python >=3.8.1,<4.0; 0.0.343 Requires-Python >=3.8.1,<4.0; 0.0.344 Requires-Python >=3.8.1,<4.0; 0.0.345 Requires-Python >=3.8.1,<4.0; 0.0.346 Requires-Python >=3.8.1,<4.0; 0.0.347 Requires-Python >=3.8.1,<4.0; 0.0.348 Requires-Python >=3.8.1,<4.0; 0.0.349 Requires-Python >=3.8.1,<4.0; 0.0.349rc1 Requires-Python >=3.8.1,<4.0; 0.0.349rc2 Requires-Python >=3.8.1,<4.0; 0.0.35 Requires-Python >=3.8.1,<4.0; 0.0.350 Requires-Python >=3.8.1,<4.0; 0.0.351 Requires-Python >=3.8.1,<4.0; 0.0.352 Requires-Python >=3.8.1,<4.0; 0.0.353 Requires-Python >=3.8.1,<4.0; 0.0.354 Requires-Python >=3.8.1,<4.0; 0.0.36 Requires-Python >=3.8.1,<4.0; 0.0.37 Requires-Python >=3.8.1,<4.0; 0.0.38 Requires-Python >=3.8.1,<4.0; 0.0.39 Requires-Python >=3.8.1,<4.0; 0.0.4 Requires-Python >=3.8.1,<4.0; 0.0.40 Requires-Python >=3.8.1,<4.0; 0.0.41 Requires-Python >=3.8.1,<4.0; 0.0.42 Requires-Python >=3.8.1,<4.0; 0.0.43 Requires-Python >=3.8.1,<4.0; 0.0.44 Requires-Python >=3.8.1,<4.0; 0.0.45 Requires-Python >=3.8.1,<4.0; 0.0.46 Requires-Python >=3.8.1,<4.0; 0.0.47 Requires-Python >=3.8.1,<4.0; 0.0.48 Requires-Python >=3.8.1,<4.0; 0.0.49 Requires-Python >=3.8.1,<4.0; 0.0.5 Requires-Python >=3.8.1,<4.0; 0.0.50 Requires-Python >=3.8.1,<4.0; 0.0.51 Requires-Python >=3.8.1,<4.0; 0.0.52 Requires-Python >=3.8.1,<4.0; 0.0.53 Requires-Python >=3.8.1,<4.0; 0.0.54 Requires-Python >=3.8.1,<4.0; 0.0.55 Requires-Python >=3.8.1,<4.0; 0.0.56 Requires-Python >=3.8.1,<4.0; 0.0.57 Requires-Python >=3.8.1,<4.0; 0.0.58 Requires-Python >=3.8.1,<4.0; 0.0.59 Requires-Python >=3.8.1,<4.0; 0.0.6 Requires-Python >=3.8.1,<4.0; 0.0.60 Requires-Python >=3.8.1,<4.0; 0.0.61 Requires-Python >=3.8.1,<4.0; 0.0.63 Requires-Python >=3.8.1,<4.0; 0.0.64 Requires-Python >=3.8.1,<4.0; 0.0.65 Requires-Python >=3.8.1,<4.0; 0.0.66 Requires-Python >=3.8.1,<4.0; 0.0.67 Requires-Python >=3.8.1,<4.0; 0.0.68 Requires-Python >=3.8.1,<4.0; 0.0.69 Requires-Python >=3.8.1,<4.0; 0.0.7 Requires-Python >=3.8.1,<4.0; 0.0.70 Requires-Python >=3.8.1,<4.0; 0.0.71 Requires-Python >=3.8.1,<4.0; 0.0.72 Requires-Python >=3.8.1,<4.0; 0.0.73 Requires-Python >=3.8.1,<4.0; 0.0.74 Requires-Python >=3.8.1,<4.0; 0.0.75 Requires-Python >=3.8.1,<4.0; 0.0.76 Requires-Python >=3.8.1,<4.0; 0.0.77 Requires-Python >=3.8.1,<4.0; 0.0.78 Requires-Python >=3.8.1,<4.0; 0.0.79 Requires-Python >=3.8.1,<4.0; 0.0.8 Requires-Python >=3.8.1,<4.0; 0.0.80 Requires-Python >=3.8.1,<4.0; 0.0.81 Requires-Python >=3.8.1,<4.0; 0.0.82 Requires-Python >=3.8.1,<4.0; 0.0.83 Requires-Python >=3.8.1,<4.0; 0.0.84 Requires-Python >=3.8.1,<4.0; 0.0.85 Requires-Python >=3.8.1,<4.0; 0.0.86 Requires-Python >=3.8.1,<4.0; 0.0.87 Requires-Python >=3.8.1,<4.0; 0.0.88 Requires-Python >=3.8.1,<4.0; 0.0.89 Requires-Python >=3.8.1,<4.0; 0.0.8rc1 Requires-Python >=3.8.1,<4.0; 0.0.90 Requires-Python >=3.8.1,<4.0; 0.0.91 Requires-Python >=3.8.1,<4.0; 0.0.92 Requires-Python >=3.8.1,<4.0; 0.0.93 Requires-Python >=3.8.1,<4.0; 0.0.94 Requires-Python >=3.8.1,<4.0; 0.0.95 Requires-Python >=3.8.1,<4.0; 0.0.96 Requires-Python >=3.8.1,<4.0; 0.0.97 Requires-Python >=3.8.1,<4.0; 0.0.98 Requires-Python >=3.8.1,<4.0; 0.0.99 Requires-Python >=3.8.1,<4.0; 0.0.99rc0 Requires-Python >=3.8.1,<4.0; 0.1.0 Requires-Python <4.0,>=3.8.1; 0.1.0 Requires-Python >=3.8.1,<4.0; 0.1.1 Requires-Python <4.0,>=3.8.1; 0.1.1 Requires-Python >=3.8.1,<4.0; 0.1.10 Requires-Python >=3.8.1,<4.0; 0.1.11 Requires-Python >=3.8.1,<4.0; 0.1.12 Requires-Python >=3.8.1,<4.0; 0.1.13 Requires-Python <4.0,>=3.8.1; 0.1.14 Requires-Python <4.0,>=3.8.1; 0.1.15 Requires-Python <4.0,>=3.8.1; 0.1.16 Requires-Python <4.0,>=3.8.1; 0.1.17 Requires-Python <4.0,>=3.8.1; 0.1.17rc1 Requires-Python <4.0,>=3.8.1; 0.1.19 Requires-Python <4.0,>=3.8.1; 0.1.2 Requires-Python <4.0,>=3.8.1; 0.1.2 Requires-Python >=3.8.1,<4.0; 0.1.20 Requires-Python <4.0,>=3.8.1; 0.1.3 Requires-Python <4.0,>=3.8.1; 0.1.3 Requires-Python >=3.8.1,<4.0; 0.1.3rc1 Requires-Python <4.0,>=3.8.1; 0.1.4 Requires-Python <4.0,>=3.8.1; 0.1.4 Requires-Python >=3.8.1,<4.0; 0.1.5 Requires-Python <4.0,>=3.8.1; 0.1.5 Requires-Python >=3.8.1,<4.0; 0.1.6 Requires-Python <4.0,>=3.8.1; 0.1.6 Requires-Python >=3.8.1,<4.0; 0.1.7 Requires-Python <4.0,>=3.8.1; 0.1.7 Requires-Python >=3.8.1,<4.0; 0.1.8 Requires-Python <4.0,>=3.8.1; 0.1.8 Requires-Python >=3.8.1,<4.0; 0.1.8rc1 Requires-Python <4.0,>=3.8.1; 0.1.9 Requires-Python >=3.8.1,<4.0; 0.2.0 Requires-Python <4.0,>=3.8.1; 0.2.0rc1 Requires-Python <4.0,>=3.8.1; 0.2.0rc2 Requires-Python <4.0,>=3.8.1; 0.2.1 Requires-Python <4.0,>=3.8.1; 0.2.2 Requires-Python <4.0,>=3.8.1; 0.2.3 Requires-Python <4.0,>=3.8.1\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-openai (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-openai\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: openai in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (1.33.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: cached-property in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (1.5.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (0.24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from httpx<1,>=0.23.0->openai) (0.17.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from pydantic<3,>=1.9.0->openai) (0.5.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from pydantic<3,>=1.9.0->openai) (6.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ATIFHANIF/.pyenv/versions/3.7.12/lib/python3.7/site-packages (from importlib-metadata->pydantic<3,>=1.9.0->openai) (3.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pip\n",
    "\n",
    "! pip install openai\n",
    "\n",
    "! pip install nltk\n",
    "\n",
    "! pip install wikipedia-api requests beautifulsoup4\n",
    "! pip install python-dotenv\n",
    "! pip install --quiet langchain openai langchain-openai unstructured\n",
    "! pip install --upgrade openai\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lj/knh84kr15sdflszx47jnzy480000gp/T/ipykernel_1994/2988442887.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "# Cell to import necessary libraries\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "nltk.download('stopwords')\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_runnable,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Updated tool for Scrapping ############################\n",
    "\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "def parse_page(html, base_url):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Extract the title for the directory name\n",
    "    title_tag = soup.find('h1')\n",
    "    title_text = title_tag.text.replace('/', '_')  # Replace '/' in titles with '_' to avoid path issues\n",
    "    \n",
    "    # Extract all text from the main content\n",
    "    content = soup.find('div', {'id': 'mw-content-text'})\n",
    "    text = content.get_text() if content else \"\"\n",
    "    \n",
    "    # Find links within the content that include the main title\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True) if a.text and title_text in a.text]\n",
    "    \n",
    "    # Convert relative links to absolute\n",
    "    links = ['https://en.wikipedia.org' + link if link.startswith('/wiki/') else link for link in links]\n",
    "    \n",
    "    return text, links, title_text\n",
    "    \n",
    "\n",
    "def scrape_wikipedia(start_url):\n",
    "    html = fetch_page(start_url)\n",
    "    text, links, title = parse_page(html, start_url)\n",
    "    \n",
    "    # For simplicity, just fetch text from the first few relevant links\n",
    "    for link in links[:10]:  # Limit to first 10 links to avoid too many requests\n",
    "        html = fetch_page(link)\n",
    "        page_text, _ , _ = parse_page(html, link)  # We don't follow further links here\n",
    "        text += \"\\n\\n\" + page_text\n",
    "\n",
    "    base_dir = os.path.join('data/scraped_data', title)\n",
    "    os.makedirs(base_dir, exist_ok=True)  # Create directory if it does not exist\n",
    "    \n",
    "    # Format current datetime for the filename\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = f\"{title}_{now}_Wikipedia.txt\"\n",
    "    filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    # Save the content to a text file\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "    return text, title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "url = 'https://en.wikipedia.org/wiki/Scent_of_a_Woman_(1992_film)'\n",
    "collected_text, title = scrape_wikipedia(url)\n",
    "print(len(collected_text))\n",
    "print(f\"Data collected for {title} and saved to the respective directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### For cleaning scrapped Data and dividing it into chunks on the basis of tokenization ############################\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Normalize whitespace and remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "def chunk_text(text, chunk_size=8000):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    # Handle the last chunk; merge with previous if too small\n",
    "    if len(chunks) > 1 and len(word_tokenize(chunks[-1])) < 1000:\n",
    "        chunks[-2] += ' ' + chunks.pop()\n",
    "    return chunks\n",
    "\n",
    "def save_chunks(chunks, base_dir, filename):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_filename = f\"{filename}_chunk{i+1}.txt\"\n",
    "        chunk_path = os.path.join(base_dir, chunk_filename)\n",
    "        with open(chunk_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(chunk)\n",
    "\n",
    "def clean_data(scraped_dir, cleaned_dir):\n",
    "    for title in os.listdir(scraped_dir):\n",
    "        title_path = os.path.join(scraped_dir, title)\n",
    "        cleaned_title_path = os.path.join(cleaned_dir, title)\n",
    "        \n",
    "        if not os.path.exists(cleaned_title_path):\n",
    "            os.makedirs(cleaned_title_path, exist_ok=True)\n",
    "        \n",
    "        for file in os.listdir(title_path):\n",
    "            file_path = os.path.join(title_path, file)\n",
    "            cleaned_file_path = os.path.join(cleaned_title_path, file.replace('.txt', ''))\n",
    "            \n",
    "            # Check if already cleaned\n",
    "            if not any(f.startswith(file.replace('.txt', '')) for f in os.listdir(cleaned_title_path)):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                # Clean the text\n",
    "                cleaned_text = normalize_text(text)\n",
    "                chunks = chunk_text(cleaned_text)\n",
    "                \n",
    "                # Save cleaned chunks\n",
    "                save_chunks(chunks, cleaned_title_path, file.replace('.txt', ''))\n",
    "                print(f\"Data cleaned and saved for {file_path}\")\n",
    "            else:\n",
    "                print(f\"Already cleaned data present for {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example usage\n",
    "# scraped_dir = 'data/scraped_data'\n",
    "# cleaned_dir = 'data/cleaned_data'\n",
    "# clean_data(scraped_dir, cleaned_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to define the function to remove unnecessary characters\n",
    "def remove_unnecessary_characters(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to define the main cleaning function\n",
    "def clean_data(text):\n",
    "    context = \"This text is a part of a formal document and should be cleaned accordingly.You are a helpful assistant. Clean and standardize the following text to make it more readable and consistent.\"\n",
    "    text = normalize_text(text)\n",
    "    text = remove_unnecessary_characters(text)\n",
    "    # text = context_based_cleaning(text,context)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to create directories for cleaned data based on title and source\n",
    "def create_cleaned_directory_structure(base_dir, title, source):\n",
    "    dir_path = os.path.join(base_dir, title, source)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to clean the scraped data and save it in the cleaned_data directory\n",
    "def clean_and_save_scraped_data(scraped_dir, cleaned_dir):\n",
    "    for root, dirs, files in os.walk(scraped_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = f.read()\n",
    "                \n",
    "                cleaned_data = clean_data(data)\n",
    "                \n",
    "                # Extract title and source from the file path\n",
    "                relative_path = os.path.relpath(file_path, scraped_dir)\n",
    "                title, source = relative_path.split(os.sep)[:2]\n",
    "                \n",
    "                cleaned_dir_path = create_cleaned_directory_structure(cleaned_dir, title, source)\n",
    "                cleaned_file_path = os.path.join(cleaned_dir_path, file)\n",
    "                \n",
    "                with open(cleaned_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(cleaned_data)\n",
    "                \n",
    "                print(f\"Cleaned data saved to: {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scraped_dir = \"data/scraped_data\"\n",
    "cleaned_dir = \"data/cleaned_data\"\n",
    "\n",
    "clean_and_save_scraped_data(scraped_dir, cleaned_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### For Chunked Data to extract Entitiy ############################\n",
    "\n",
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize ChatOpenAI with the model and API key\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "def read_files_from_directory(directory_path: str):\n",
    "    \"\"\"\n",
    "    Read text files from the specified directory.\n",
    "    \"\"\"\n",
    "    files_content = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    files_content[file_path] = file.read()\n",
    "    print(f\"Read {len(files_content)} files from directory {directory_path}\")\n",
    "    return files_content\n",
    "\n",
    "\n",
    "def extract_information(llm, text: str, title: str) -> dict:\n",
    "    prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "        I am analyzing the TV show '{title}' and need to extract detailed information about characters to understand plot dynamics better. Please list all identifiable characters from the text below. For each character, provide:\n",
    "        - A short fragment of text that captures their essence or a significant action.\n",
    "        - Potential aliases.\n",
    "        Fragments should not mention lists of character names from the TV show or movie. Provide the smallest fragment with contextual meaning.\n",
    "        The response should be concise and structured in JSON format to facilitate relationship analysis in future iterations.\n",
    "\n",
    "        An example of a valid response:\n",
    "        [\n",
    "            {{\n",
    "                \"character\": \"Jon Snow\",\n",
    "                \"aliases\": [\"Lord Snow\", \"The White Wolf\"],\n",
    "                \"fragment\": \"Jon Snow pledges his life to the Night's Watch and refuses to leave even when tempted.\"\n",
    "            }},\n",
    "            {{\n",
    "                \"character\": \"Daenerys Targaryen\",\n",
    "                \"aliases\": [\"Dany\", \"Khaleesi\"],\n",
    "                \"fragment\": \"Daenerys sets sail for Westeros with her armies and dragons, aiming to reclaim her family's throne.\"\n",
    "            }}\n",
    "        ]\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    user_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Below is part of that Wikipedia page on the TV show {title}.\\n\\n{text}\"\n",
    "    }\n",
    "\n",
    "    combined_prompts = [prompt, user_prompt]  # This should be a list of dictionaries\n",
    "\n",
    "    response = llm.invoke(combined_prompts)\n",
    "    try:\n",
    "        response_json = json.loads(response.content)\n",
    "        return response_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        print(f\"Received malformed JSON response: {response.content}\")\n",
    "        return []  # Return an empty list to indicate failure\n",
    "\n",
    "\n",
    "def save_json_to_file(data: dict, file_path: str):\n",
    "    \"\"\"\n",
    "    Save the JSON data to a file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# def process_cleaned_data(directory_path: str, title: str = None, is_file: bool = False):\n",
    "#     \"\"\"\n",
    "#     Process all cleaned data files and extract information.\n",
    "#     \"\"\"\n",
    "#     isProcess =True\n",
    "#     if is_file:\n",
    "#         title = title or os.path.basename(directory_path).split('.')[0]\n",
    "#         entity_json_path = f\"data/Entity_json/{title}.json\"\n",
    "#         if os.path.exists(entity_json_path):\n",
    "#             user_input = input(f\"The title '{title}' is already in Entity_json. Would you like to process it again? (yes/no): \")\n",
    "#             if user_input.lower() != 'yes':\n",
    "#                 print(f\"Skipping processing for {title}.\")\n",
    "#                 return\n",
    "#         content = read_files_from_directory(directory_path)\n",
    "#         extracted_info = extract_information(llm, content, title)\n",
    "#         save_json_to_file(extracted_info, entity_json_path)\n",
    "#         print(f\"Processed and saved: {entity_json_path}\")\n",
    "#     else:\n",
    "#         # Process a directory\n",
    "#         files_content = read_files_from_directory(directory_path)\n",
    "#         if not files_content:\n",
    "#             print(\"No files found in the directory.\")\n",
    "#             return\n",
    "#         for filepath, content in files_content.items():\n",
    "#             title = os.path.basename(filepath).split('_')[0]  # Derive title from filename\n",
    "#             print(title)\n",
    "#             entity_json_path = f\"data/Entity_json/{title}\"\n",
    "#             print(entity_json_path)\n",
    "#             if os.path.exists(entity_json_path):\n",
    "                \n",
    "#                 user_input = input(f\"The title '{title}' is already in Entity_json. Would you like to process it again? (yes/no): \")\n",
    "#                 if user_input.lower() != 'yes':\n",
    "#                     print(f\"Skipping processing for {title}.\")\n",
    "#                     continue\n",
    "                 \n",
    "#             # extracted_info = extract_information(llm, content, title)\n",
    "#             # save_json_to_file(extracted_info, entity_json_path)\n",
    "#             print(f\"Processed and saved: {entity_json_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_cleaned_data(cleaned_data_dir: str):\n",
    "    \"\"\"\n",
    "    Process all cleaned data directories and extract information.\n",
    "    \"\"\"\n",
    "    for title in os.listdir(cleaned_data_dir):\n",
    "        title_dir_path = os.path.join(cleaned_data_dir, title)\n",
    "        entity_json_dir_path = os.path.join(\"data/Entity_json\", title)\n",
    "        \n",
    "        if os.path.exists(entity_json_dir_path):\n",
    "            user_input = input(f\"The title '{title}' is already in Entity_json. Would you like to process it again? (yes/no): \")\n",
    "            if user_input.lower() != 'yes':\n",
    "                print(f\"Skipping processing for all files under the title '{title}'.\")\n",
    "                continue\n",
    "            else:\n",
    "                # Remove existing JSON files to replace with new ones\n",
    "                for file in os.listdir(entity_json_dir_path):\n",
    "                    if file.endswith('.json'):\n",
    "                        os.remove(os.path.join(entity_json_dir_path, file))\n",
    "\n",
    "        files_content = read_files_from_directory(title_dir_path)\n",
    "        for filepath, content in files_content.items():\n",
    "            extracted_info = extract_information(llm, content, title)\n",
    "            json_filename = os.path.basename(filepath).replace(\".txt\", \".json\")\n",
    "            json_filepath = os.path.join(entity_json_dir_path, json_filename)\n",
    "            save_json_to_file(extracted_info, json_filepath)\n",
    "            print(f\"Processed and saved: {json_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# Process a directory\n",
    "process_cleaned_data(\"data/cleaned_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### For CHunked Data to extract Events ############################\n",
    "\n",
    "def extract_narrative_elements(llm, text: str, title: str) -> dict:\n",
    "    prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "        I am analyzing the TV show '{title}' for the first time and need to extract detailed information about key narrative elements such as objects, locations, addresses, scenes, and events. Please identify these elements from the text below and provide:\n",
    "        - A concise description or identification of the element.\n",
    "        - A list of excerpts that discuss this element, ensuring each excerpt provides meaningful context.\n",
    "\n",
    "        The response should be concise and structured in JSON format to facilitate analysis and visualization of these narrative elements in future iterations.\n",
    "\n",
    "        An example of a valid response:\n",
    "        [\n",
    "            {{\n",
    "                \"element\": \"Shooting scene\",\n",
    "                \"description\": \"Episode 3, Season 4\",\n",
    "                \"excerpts\": [\n",
    "                    \"The intense shooting scene in the third episode of the fourth season was pivotal to the plot development.\",\n",
    "                    \"During the shootout, the main character's dilemma comes to a head, forcing a decision that changes the course of the story.\"\n",
    "                ]\n",
    "            }},\n",
    "            {{\n",
    "                \"element\": \"Central Park\",\n",
    "                \"description\": \"Location\",\n",
    "                \"excerpts\": [\n",
    "                    \"Several key discussions between the protagonists occur in Central Park, serving as a backdrop to their evolving relationships.\",\n",
    "                    \"Central Park is depicted in multiple scenes as a place of reflection and confrontation among the characters.\"\n",
    "                ]\n",
    "            }}\n",
    "        ]\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    user_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Below is part of that Wikipedia page on the TV show {title}.\\n\\n{text}\"\n",
    "    }\n",
    "\n",
    "    combined_prompts = [prompt, user_prompt]  # This should be a list of dictionaries\n",
    "\n",
    "    response = llm.invoke(combined_prompts)\n",
    "    try:\n",
    "        response_json = json.loads(response.content)\n",
    "        return response_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        print(f\"Received malformed JSON response: {response.content}\")\n",
    "        return []  # Return an empty list to indicate failure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_cleaned_data_Events(directory_path: str, title: str = None, is_file: bool = False):\n",
    "    \"\"\"Process all cleaned data files and extract narrative information.\"\"\"\n",
    "    if is_file:\n",
    "        with open(directory_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        title = title or os.path.basename(directory_path).split('.')[0]\n",
    "        extracted_info = extract_narrative_elements(llm, content, title)\n",
    "        json_filename = os.path.basename(directory_path).replace(\".txt\", \".json\")\n",
    "        json_filepath = os.path.join(\"data/Events_json\", json_filename)\n",
    "        save_json_to_file(extracted_info, json_filepath)\n",
    "        print(f\"Processed and saved: {json_filepath}\")\n",
    "    else:\n",
    "        files_content = read_files_from_directory(directory_path)\n",
    "        if not files_content:\n",
    "            print(\"No files found in the directory.\")\n",
    "            return\n",
    "        for filepath, content in files_content.items():\n",
    "            title = os.path.basename(filepath).split('_')[0]  # Derive title from filename\n",
    "            extracted_info = extract_narrative_elements(llm, content, title)\n",
    "            json_filename = os.path.basename(filepath).replace(\".txt\", \".json\")\n",
    "            json_filepath = os.path.join(\"data/Events_json\", title, json_filename)\n",
    "            save_json_to_file(extracted_info, json_filepath)\n",
    "            print(f\"Processed and saved: {json_filepath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "process_cleaned_data_Events(\"data/cleaned_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Merge Entities ##############################\n",
    "\n",
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the language model\n",
    "\n",
    "def merge_json_files(directory_path):\n",
    "    \"\"\"Merge all JSON files in a directory into a single JSON list.\"\"\"\n",
    "    merged_data = []\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"No directory found at {directory_path}\")\n",
    "        return merged_data\n",
    "\n",
    "    for file in os.listdir(directory_path):\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(directory_path, file), 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    if data:  # Ensure that data is not empty\n",
    "                        merged_data.extend(data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from file {file}: {e}\")\n",
    "    return merged_data\n",
    "\n",
    "def process_title_entities(title_directory):\n",
    "    \"\"\"Process all entity JSON files for a given title.\"\"\"\n",
    "    merged_data = merge_json_files(title_directory)\n",
    "    if not merged_data:\n",
    "        print(\"No data found to merge.\")\n",
    "        return\n",
    "\n",
    "    title = os.path.basename(title_directory)\n",
    "    # Save the merged data\n",
    "    merged_dir = os.path.join('data/Merged_Entity_json', title)\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    merged_file_path = os.path.join(merged_dir, f\"{title}_Merged.json\")\n",
    "    with open(merged_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, indent=4)\n",
    "    print(f\"Merged data saved to {merged_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "title_directory = 'data/Entity_json/Interstellar (film)'  # Specify the subfolder for a specific title\n",
    "process_title_entities(title_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### For Deduping without LLM ##############################\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def write_json(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def merge_entries(entries):\n",
    "    # Dictionary to hold merged characters\n",
    "    merged_characters = {}\n",
    "\n",
    "    for entry in entries:\n",
    "        # Identify character key\n",
    "        character_key = entry['character'].lower()\n",
    "        \n",
    "        # If the character is already added, merge the data\n",
    "        if character_key in merged_characters:\n",
    "            existing_entry = merged_characters[character_key]\n",
    "            existing_entry['aliases'] = list(set(existing_entry['aliases'] + entry['aliases']))\n",
    "            existing_entry['fragment'].append(entry['fragment'])\n",
    "        else:\n",
    "            # Otherwise, add new entry\n",
    "            merged_characters[character_key] = {\n",
    "                'character': entry['character'],\n",
    "                'aliases': entry['aliases'],\n",
    "                'fragment': [entry['fragment']]\n",
    "            }\n",
    "\n",
    "    # Convert merged data to list\n",
    "    return list(merged_characters.values())\n",
    "\n",
    "def process_json_file(input_file_path, output_file_path):\n",
    "    # Read data from the input JSON file\n",
    "    entries = read_json(input_file_path)\n",
    "    \n",
    "    # Merge entries\n",
    "    merged_data = merge_entries(entries)\n",
    "    \n",
    "    # Write merged data to the output JSON file\n",
    "    write_json(merged_data, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage\n",
    "input_file_path = 'data/Merged_Entity_json/Interstellar (film)/Interstellar (film)_Merged.json'\n",
    "output_file_path = 'data/Deduped_Entity_json/Interstellar/Interstellar_Deduped.json'\n",
    "\n",
    "process_json_file(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest deduping usinf llm SEMANTIC DEDUPING working code\n",
    "from openai import OpenAI\n",
    "import re\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # print(data)\n",
    "    return data\n",
    "\n",
    "# Function to write to a JSON file\n",
    "def write_json(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def process_json_with_openai(title, json_input):\n",
    "    # completion = client.chat.completions.create(\n",
    "\n",
    "    #     messages=[\n",
    "    #         {\n",
    "    #             \"role\": \"system\",\n",
    "    #             \"content\": \"You are a helpful assistant skilled in understanding complex character relationships and providing guidance on merging similar characters into single entries. Given a JSON object list of characters, their aliases, and descriptions(fragments), identify duplicates and suggest how their data might be merged. The aim is to reduce duplication and create a cohesive character profile for each unique individual. Output the deduped JSON Object. Make semantic deduping.\"\n",
    "    #         },\n",
    "\n",
    "    #         {\n",
    "    #             \"role\": \"user\",\n",
    "    #             \"content\": f\"The title of the show is '{title}'. Here is the character data: {json_input}. For now return a json onject with removed duplication, also concatinate there fragments and aliases if found sematically same\",\n",
    "    #         }\n",
    "    #     ],\n",
    "    #     model=\"gpt-3.5-turbo-16k\"\n",
    "    # )\n",
    "    completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant skilled in understanding complex character relationships and providing guidance on merging similar characters into single entries. Given a JSON object list of characters, their aliases, and descriptions(fragments), identify duplicates and suggest how their data might be merged. The aim is to reduce duplication and create a cohesive character profile for each unique individual. Output the deduped JSON Object. Make semantic deduping.\"\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"The title of the show is '{title}'. Here is the character data: {json_input}. For now return a json onject with removed duplication, also concatinate there fragments if found similar character\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-16k\"\n",
    "    )\n",
    "    res=completion.choices[0].message.content\n",
    "    # print(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file_path = 'data/Merged_Entity_json/Interstellar (film)/Interstellar (film)_Merged.json'  # Path to your input JSON file\n",
    "output_file_path = 'data/Deduped_Entity_json/Interstellar/Interstellar_Desuped_llm3.json'  # Path to save the output JSON file\n",
    "\n",
    "# Read the input JSON file\n",
    "input_data = read_json(input_file_path)\n",
    "title = \"Interstellar\"\n",
    "# Process the JSON with OpenAI\n",
    "output_data = process_json_with_openai(title,(input_data))\n",
    "print(output_data)\n",
    "\n",
    "json_array_match = re.search(r'\\[\\s*\\{[\\s\\S]*?\\}\\s*\\]', output_data)\n",
    "if json_array_match:\n",
    "    json_array_str = json_array_match.group(0)\n",
    "    print(\"Extracted JSON Array String:\\n\", json_array_str)\n",
    "else:\n",
    "    print(\"No JSON array found in the response\")\n",
    "    json_array_str = '[]'  # Fallback to an empty list if no JSON is found\n",
    "\n",
    "# Parse the extracted JSON array\n",
    "try:\n",
    "    parsed_data = json.loads(json_array_str)\n",
    "    print(\"Parsed Data:\\n\", parsed_data)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSONDecodeError: {e}\")\n",
    "    parsed_data = []\n",
    "\n",
    "\n",
    "# Write the output JSON file\n",
    "write_json(output_file_path, parsed_data)\n",
    "\n",
    "print(f\"Processed JSON has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('3.7.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1d9f46e0be80a03f6a545502f84140f266df46f611c430add7280143a3c88aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

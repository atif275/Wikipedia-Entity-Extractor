{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import tiktoken\n",
    "from urllib.parse import unquote\n",
    "from urllib.request import urlopen\n",
    "# Cell to import necessary libraries\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_runnable,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### For Non Chunked Data ############################\n",
    "\n",
    "\n",
    "\n",
    "def read_files_from_directory(directory_path: str):\n",
    "    \"\"\"\n",
    "    Read text files from the specified directory.\n",
    "\n",
    "    :param directory_path: The path to the directory containing text files.\n",
    "    :return: A dictionary with file names as keys and file contents as values.\n",
    "    \"\"\"\n",
    "    files_content = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    files_content[file_path] = file.read()\n",
    "    print(f\"Read {len(files_content)} files from directory {directory_path}\")\n",
    "    return files_content\n",
    "\n",
    "def split_text(text, max_tokens):\n",
    "    \"\"\"\n",
    "    Split text into smaller chunks based on token limits.\n",
    "\n",
    "    :param text: The text to be split.\n",
    "    :param max_tokens: The maximum number of tokens per chunk.\n",
    "    :return: A list of text chunks.\n",
    "    \"\"\"\n",
    "    enc = tiktoken.get_encoding('gpt2')\n",
    "    tokens = enc.encode(text)\n",
    "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    text_chunks = [enc.decode(chunk) for chunk in chunks]\n",
    "    return text_chunks\n",
    "\n",
    "def extract_information(text: str, title: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract information from the text using the LLM.\n",
    "\n",
    "    :param text: The text to be processed.\n",
    "    :param title: The title of the TV show.\n",
    "    :return: A JSON object with extracted information.\n",
    "    \"\"\"\n",
    "    max_tokens_per_chunk = 5000 - 385  # Leave room for prompt tokens\n",
    "    text_chunks = split_text(text, max_tokens_per_chunk)\n",
    "\n",
    "    combined_results = []\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"I want you to imagine you are reading a text page on the TV show {title} for the first time and have no prior knowledge of TV show {title}.\n",
    "After reading that text -- I want you to extract a list of people that you can identify from that piece of text. I also want you to give me the smallest fragment of text where you identified that person in the given text. Note there may be multiple references to a person you identify and I want you to list all of them. I also want you to give me potential aliases for each person.\n",
    "Return your answers in a JSON object. The returned JSON object should be a list of references objects. Where the reference objects are discretionary of the person and text fragments where the person was identified.\n",
    "\n",
    "An example of a valid response fragment is as follows:\n",
    "\n",
    "[\n",
    "    {{\n",
    "        \"person\": \"Walter White\",\n",
    "        \"aliases\": [\n",
    "            \"Walt\",\n",
    "            \"Walter\"\n",
    "       ]\n",
    "        \"fragments\": [\n",
    "            \"Walter White settles into his new surroundings and takes a liking to his new lab assistant Gale Boetticher.\",\n",
    "            \"Marie Schrader, who suggests that he ask Walt about it, due to Walt's previous association with Jesse.\",\n",
    "            \"He tells them again that he will not allow them to kill Walt until his business with him has concluded, but gives them his blessing to instead go after the man who actually pulled the trigger on Tuco Salamanca: Hank.\"\n",
    "        ]\n",
    "    }},\n",
    "    {{\n",
    "        \"person\": \"Gale Boetticher\",\n",
    "       \"aliases\": [\"Gale\"]\n",
    "        \"fragments\": [\n",
    "            \"Walter White settles into his new surroundings and takes a liking to his new lab assistant Gale Boetticher.\"\n",
    "        ]\n",
    "    }}\n",
    "]\n",
    "Below is part of that wikipedia page on the TV show {title}.\n",
    "\n",
    "\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Below is part of that Wikipedia page on the TV show {title}.\\n\\n{chunk}\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        chunk_results = json.loads(response.content)\n",
    "        combined_results.extend(chunk_results)\n",
    "\n",
    "    # Combine results by merging fragments and aliases for the same person\n",
    "    merged_results = {}\n",
    "    for item in combined_results:\n",
    "        person = item['person']\n",
    "        if person not in merged_results:\n",
    "            merged_results[person] = {\n",
    "                'person': person,\n",
    "                'aliases': set(item['aliases']),\n",
    "                'fragments': set(item['fragments'])\n",
    "            }\n",
    "        else:\n",
    "            merged_results[person]['aliases'].update(item['aliases'])\n",
    "            merged_results[person]['fragments'].update(item['fragments'])\n",
    "\n",
    "    # Convert sets back to lists\n",
    "    for person in merged_results:\n",
    "        merged_results[person]['aliases'] = list(merged_results[person]['aliases'])\n",
    "        merged_results[person]['fragments'] = list(merged_results[person]['fragments'])\n",
    "\n",
    "    return list(merged_results.values())\n",
    "\n",
    "def save_json_to_file(data: dict, file_path: str):\n",
    "    \"\"\"\n",
    "    Save the JSON data to a file.\n",
    "\n",
    "    :param data: The data to be saved.\n",
    "    :param file_path: The path to the file where the data will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def process_cleaned_data(directory_path: str, title: str = None, is_file: bool = False):\n",
    "    \"\"\"\n",
    "    Process all cleaned data files and extract information.\n",
    "\n",
    "    :param directory_path: The path to the directory containing cleaned text files or the path to a specific file.\n",
    "    :param title: The title of the TV show (if known).\n",
    "    :param is_file: A boolean indicating whether the path is a file.\n",
    "    \"\"\"\n",
    "    if is_file:\n",
    "        # Process a single file\n",
    "        with open(directory_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        title = title or os.path.basename(directory_path).split('.')[0]\n",
    "        extracted_info = extract_information(content, title)\n",
    "        json_filename = os.path.basename(directory_path).replace(\".txt\", \".json\")\n",
    "        json_filepath = os.path.join(\"data/Entity_json\", json_filename)\n",
    "        save_json_to_file(extracted_info, json_filepath)\n",
    "        print(f\"Processed and saved: {json_filepath}\")\n",
    "    else:\n",
    "        # Process a directory\n",
    "        files_content = read_files_from_directory(directory_path)\n",
    "        if not files_content:\n",
    "            print(\"No files found in the directory.\")\n",
    "            return\n",
    "        for filepath, content in files_content.items():\n",
    "            # Derive title from directory structure if not provided\n",
    "            relative_path = os.path.relpath(filepath, directory_path)\n",
    "            path_parts = relative_path.split(os.sep)\n",
    "            title = path_parts[0]  # Assuming title is the first part of the path\n",
    "            source = path_parts[1]  # Assuming source is the second part of the path\n",
    "            extracted_info = extract_information(content, title)\n",
    "            json_filename = os.path.basename(filepath).replace(\".txt\", \".json\")\n",
    "            json_filepath = os.path.join(\"data/Entity_json\", title, source, json_filename)\n",
    "            save_json_to_file(extracted_info, json_filepath)\n",
    "            print(f\"Processed and saved: {json_filepath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Process a single file\n",
    "# process_cleaned_data(\"cleaned_data/Breaking_Bad/wikipedia/sample.txt\", title=\"Breaking Bad\", is_file=True)\n",
    "\n",
    "# Process a directory\n",
    "process_cleaned_data(\"data/cleaned_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

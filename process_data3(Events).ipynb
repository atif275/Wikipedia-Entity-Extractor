{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import tiktoken\n",
    "from urllib.parse import unquote\n",
    "from urllib.request import urlopen\n",
    "# Cell to import necessary libraries\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_runnable,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "# Initialize ChatOpenAI with the model and API key\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_from_directory(directory_path: str):\n",
    "    \"\"\"\n",
    "    Read text files from the specified directory.\n",
    "    \"\"\"\n",
    "    files_content = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    files_content[file_path] = file.read()\n",
    "    print(f\"Read {len(files_content)} files from directory {directory_path}\")\n",
    "    return files_content\n",
    "\n",
    "\n",
    "\n",
    "def save_json_to_file(data: dict, file_path: str):\n",
    "    \"\"\"\n",
    "    Save the JSON data to a file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_narrative_elements(llm, text: str, title: str) -> dict:\n",
    "    prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "        I am analyzing the TV show '{title}' for the first time and need to extract detailed information about key narrative elements such as objects, locations, addresses, scenes, and events. Please identify these elements from the text below and provide:\n",
    "        - A concise description or identification of the element.\n",
    "        - A list of excerpts that discuss this element, ensuring each excerpt provides meaningful context.\n",
    "\n",
    "        The response should be concise and structured in JSON format to facilitate analysis and visualization of these narrative elements in future iterations.\n",
    "\n",
    "        An example of a valid response:\n",
    "        [\n",
    "            {{\n",
    "                \"element\": \"Shooting scene\",\n",
    "                \"description\": \"Episode 3, Season 4\",\n",
    "                \"excerpts\": [\n",
    "                    \"The intense shooting scene in the third episode of the fourth season was pivotal to the plot development.\",\n",
    "                    \"During the shootout, the main character's dilemma comes to a head, forcing a decision that changes the course of the story.\"\n",
    "                ]\n",
    "            }},\n",
    "            {{\n",
    "                \"element\": \"Central Park\",\n",
    "                \"description\": \"Location\",\n",
    "                \"excerpts\": [\n",
    "                    \"Several key discussions between the protagonists occur in Central Park, serving as a backdrop to their evolving relationships.\",\n",
    "                    \"Central Park is depicted in multiple scenes as a place of reflection and confrontation among the characters.\"\n",
    "                ]\n",
    "            }}\n",
    "        ]\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    user_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Below is part of that Wikipedia page on the TV show {title}.\\n\\n{text}\"\n",
    "    }\n",
    "\n",
    "    combined_prompts = [prompt, user_prompt]  # This should be a list of dictionaries\n",
    "\n",
    "    response = llm.invoke(combined_prompts)\n",
    "    try:\n",
    "        response_json = json.loads(response.content)\n",
    "        return response_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        print(f\"Received malformed JSON response: {response.content}\")\n",
    "        return []  # Return an empty list to indicate failure\n",
    "\n",
    "\n",
    "\n",
    "def process_cleaned_data_Events(directory_path: str, title: str = None, is_file: bool = False):\n",
    "    \"\"\"Process all cleaned data files and extract narrative information.\"\"\"\n",
    "    if is_file:\n",
    "        with open(directory_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        title = title or os.path.basename(directory_path).split('.')[0]\n",
    "        extracted_info = extract_narrative_elements(llm, content, title)\n",
    "        json_filename = os.path.basename(directory_path).replace(\".txt\", \".json\")\n",
    "        json_filepath = os.path.join(\"data/Events_json\", json_filename)\n",
    "        save_json_to_file(extracted_info, json_filepath)\n",
    "        print(f\"Processed and saved: {json_filepath}\")\n",
    "    else:\n",
    "        files_content = read_files_from_directory(directory_path)\n",
    "        if not files_content:\n",
    "            print(\"No files found in the directory.\")\n",
    "            return\n",
    "        for filepath, content in files_content.items():\n",
    "            title = os.path.basename(filepath).split('_')[0]  # Derive title from filename\n",
    "            extracted_info = extract_narrative_elements(llm, content, title)\n",
    "            json_filename = os.path.basename(filepath).replace(\".txt\", \".json\")\n",
    "            json_filepath = os.path.join(\"data/Events_json\", title, json_filename)\n",
    "            save_json_to_file(extracted_info, json_filepath)\n",
    "            print(f\"Processed and saved: {json_filepath}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Usage\n",
    "process_cleaned_data_Events(\"data/cleaned_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

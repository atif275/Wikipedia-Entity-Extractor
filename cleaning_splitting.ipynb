{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import tiktoken\n",
    "from urllib.parse import unquote\n",
    "from urllib.request import urlopen\n",
    "# Cell to import necessary libraries\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_runnable,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### For cleaning scrapped Dat and dividing it into chunks on the basis of tokenization ############################\n",
    "\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Normalize whitespace and remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "def chunk_text(text, chunk_size=4000):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    # Handle the last chunk; merge with previous if too small\n",
    "    if len(chunks) > 1 and len(word_tokenize(chunks[-1])) < 1000:\n",
    "        chunks[-2] += ' ' + chunks.pop()\n",
    "    return chunks\n",
    "\n",
    "def save_chunks(chunks, base_dir, filename):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_filename = f\"{filename}_chunk{i+1}.txt\"\n",
    "        chunk_path = os.path.join(base_dir, chunk_filename)\n",
    "        with open(chunk_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(chunk)\n",
    "\n",
    "def clean_data(scraped_dir, cleaned_dir):\n",
    "    for title in os.listdir(scraped_dir):\n",
    "        title_path = os.path.join(scraped_dir, title)\n",
    "        cleaned_title_path = os.path.join(cleaned_dir, title)\n",
    "        \n",
    "        if not os.path.exists(cleaned_title_path):\n",
    "            os.makedirs(cleaned_title_path, exist_ok=True)\n",
    "        \n",
    "        for file in os.listdir(title_path):\n",
    "            file_path = os.path.join(title_path, file)\n",
    "            cleaned_file_path = os.path.join(cleaned_title_path, file.replace('.txt', ''))\n",
    "            \n",
    "            # Check if already cleaned\n",
    "            if not any(f.startswith(file.replace('.txt', '')) for f in os.listdir(cleaned_title_path)):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                # Clean the text\n",
    "                cleaned_text = normalize_text(text)\n",
    "                chunks = chunk_text(cleaned_text)\n",
    "                \n",
    "                # Save cleaned chunks\n",
    "                save_chunks(chunks, cleaned_title_path, file.replace('.txt', ''))\n",
    "                print(f\"Data cleaned and saved for {file_path}\")\n",
    "            else:\n",
    "                print(f\"Already cleaned data present for {file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "scraped_dir = 'data/scraped_data'\n",
    "cleaned_dir = 'data/cleaned_data'\n",
    "clean_data(scraped_dir, cleaned_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to define the text normalization function\n",
    "def normalize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to define the function to remove unnecessary characters\n",
    "def remove_unnecessary_characters(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to define the context-based cleaning function using LangChain and OpenAI API\n",
    "\n",
    "\n",
    "def context_based_cleaning(input_text: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text based on the given context using ChatOpenAI.\n",
    "\n",
    "    :param input_text: The text to be cleaned.\n",
    "    :param context: The context to guide the cleaning process.\n",
    "    :return: The cleaned text.\n",
    "    \"\"\"\n",
    "    def split_text(text, max_tokens):\n",
    "        # Tokenize the text using tiktoken\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        \n",
    "        # Split the tokens into chunks of max_tokens\n",
    "        chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "        \n",
    "        # Decode the token chunks back to text\n",
    "        text_chunks = [enc.decode(chunk) for chunk in chunks]\n",
    "        \n",
    "        return text_chunks\n",
    "\n",
    "    max_tokens_per_chunk = 5000 - 385  # Leaving room for prompt tokens\n",
    "\n",
    "    # Split the input text into manageable chunks\n",
    "    text_chunks = split_text(input_text, max_tokens_per_chunk)\n",
    "\n",
    "    cleaned_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a text cleaning assistant. Your task is to clean the input text based on the provided context.\n",
    "    ## Context\n",
    "    {context}\n",
    "    ## Instructions\n",
    "    - Remove irrelevant information.\n",
    "    - Correct grammatical errors.\n",
    "    - Ensure the text is clear and concise.\n",
    "    - Maintain the original meaning as much as possible.\n",
    "    ## Output\n",
    "    Provide the cleaned version of the text in the same language as the input.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Clean the following text based on the context:\\n\\n{chunk}\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = llm.invoke(messages)\n",
    "        cleaned_chunks.append(response.content)\n",
    "\n",
    "    # Concatenate the cleaned chunks to get the full cleaned text\n",
    "    cleaned_text = \"\\n\".join(cleaned_chunks)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# input_text = \"Ths is a smple textt with somee speling mistkes and extraneous infformation dsfsdda44 54454 sfdfad.;;';.\"\n",
    "# context = \"This text is a part of a formal document and should be cleaned accordingly.\"\n",
    "\n",
    "# cleaned_text = clean_text(input_text, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to define the main cleaning function\n",
    "def clean_data(text):\n",
    "    context = \"This text is a part of a formal document and should be cleaned accordingly.You are a helpful assistant. Clean and standardize the following text to make it more readable and consistent.\"\n",
    "    text = normalize_text(text)\n",
    "    text = remove_unnecessary_characters(text)\n",
    "    # text = context_based_cleaning(text,context)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to create directories for cleaned data based on title and source\n",
    "def create_cleaned_directory_structure(base_dir, title, source):\n",
    "    dir_path = os.path.join(base_dir, title, source)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to clean the scraped data and save it in the cleaned_data directory\n",
    "def clean_and_save_scraped_data(scraped_dir, cleaned_dir):\n",
    "    for root, dirs, files in os.walk(scraped_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = f.read()\n",
    "                \n",
    "                cleaned_data = clean_data(data)\n",
    "                \n",
    "                # Extract title and source from the file path\n",
    "                relative_path = os.path.relpath(file_path, scraped_dir)\n",
    "                title, source = relative_path.split(os.sep)[:2]\n",
    "                \n",
    "                cleaned_dir_path = create_cleaned_directory_structure(cleaned_dir, title, source)\n",
    "                cleaned_file_path = os.path.join(cleaned_dir_path, file)\n",
    "                \n",
    "                with open(cleaned_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(cleaned_data)\n",
    "                \n",
    "                print(f\"Cleaned data saved to: {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Example usage:\n",
    "\n",
    "scraped_dir = \"data/scraped_data\"\n",
    "cleaned_dir = \"data/cleaned_data\"\n",
    "\n",
    "clean_and_save_scraped_data(scraped_dir, cleaned_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

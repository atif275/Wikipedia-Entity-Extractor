{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import tiktoken\n",
    "from urllib.parse import unquote\n",
    "from urllib.request import urlopen\n",
    "# Cell to import necessary libraries\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_runnable,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "# Initialize ChatOpenAI with the model and API key\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### For Chunked Data ############################\n",
    "\n",
    "def read_files_from_directory(directory_path: str):\n",
    "    \"\"\"\n",
    "    Read text files from the specified directory.\n",
    "    \"\"\"\n",
    "    files_content = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    files_content[file_path] = file.read()\n",
    "    print(f\"Read {len(files_content)} files from directory {directory_path}\")\n",
    "    return files_content\n",
    "\n",
    "\n",
    "def extract_information(llm, text: str, title: str) -> dict:\n",
    "    prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "        I am analyzing the TV show '{title}' and need to extract detailed information about characters to understand plot dynamics better. Please list all identifiable characters from the text below. For each character, provide:\n",
    "        - A short fragment of text that captures their essence or a significant action.\n",
    "        - Potential aliases.\n",
    "        Fragments should not mention lists of character names from the TV show or movie. Provide the smallest fragment with contextual meaning.\n",
    "        The response should be concise and structured in JSON format to facilitate relationship analysis in future iterations.\n",
    "\n",
    "        An example of a valid response:\n",
    "        [\n",
    "            {{\n",
    "                \"character\": \"Jon Snow\",\n",
    "                \"aliases\": [\"Lord Snow\", \"The White Wolf\"],\n",
    "                \"fragment\": \"Jon Snow pledges his life to the Night's Watch and refuses to leave even when tempted.\"\n",
    "            }},\n",
    "            {{\n",
    "                \"character\": \"Daenerys Targaryen\",\n",
    "                \"aliases\": [\"Dany\", \"Khaleesi\"],\n",
    "                \"fragment\": \"Daenerys sets sail for Westeros with her armies and dragons, aiming to reclaim her family's throne.\"\n",
    "            }}\n",
    "        ]\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    user_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Below is part of that Wikipedia page on the TV show {title}.\\n\\n{text}\"\n",
    "    }\n",
    "\n",
    "    combined_prompts = [prompt, user_prompt]  # This should be a list of dictionaries\n",
    "\n",
    "    response = llm.invoke(combined_prompts)\n",
    "    try:\n",
    "        response_json = json.loads(response.content)\n",
    "        return response_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        print(f\"Received malformed JSON response: {response.content}\")\n",
    "        return []  # Return an empty list to indicate failure\n",
    "\n",
    "\n",
    "def save_json_to_file(data: dict, file_path: str):\n",
    "    \"\"\"\n",
    "    Save the JSON data to a file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def process_cleaned_data(directory_path: str, title: str = None, is_file: bool = False):\n",
    "    \"\"\"\n",
    "    Process all cleaned data files and extract information.\n",
    "    \"\"\"\n",
    "    if is_file:\n",
    "        # Process a single file\n",
    "        with open(directory_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        title = title or os.path.basename(directory_path).split('.')[0]\n",
    "        print(title)\n",
    "        # print(content)\n",
    "        extracted_info = extract_information(llm,content, title)\n",
    "        json_filename = os.path.basename(directory_path).replace(\".txt\", \".json\")\n",
    "        json_filepath = os.path.join(\"data/Entity_json\", json_filename)\n",
    "        save_json_to_file(extracted_info, json_filepath)\n",
    "        print(f\"Processed and saved: {json_filepath}\")\n",
    "    else:\n",
    "        # Process a directory\n",
    "        files_content = read_files_from_directory(directory_path)\n",
    "        if not files_content:\n",
    "            print(\"No files found in the directory.\")\n",
    "            return\n",
    "        for filepath, content in files_content.items():\n",
    "            title = os.path.basename(filepath).split('_')[0]  # Derive title from filename\n",
    "            print(\"fdfdsfndsnfsdjkfnsdkjfnsdknf\")\n",
    "            print(title)\n",
    "            # print(content)\n",
    "            extracted_info = extract_information(llm,content, title)\n",
    "            json_filename = os.path.basename(filepath).replace(\".txt\", \".json\")\n",
    "            json_filepath = os.path.join(\"data/Entity_json\", title, json_filename)\n",
    "            save_json_to_file(extracted_info, json_filepath)\n",
    "            print(f\"Processed and saved: {json_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# Process a directory\n",
    "process_cleaned_data(\"data/cleaned_data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba28974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in /opt/anaconda3/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.11/site-packages (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Cell to install necessary packages\n",
    "! pip install wikipedia-api requests beautifulsoup4\n",
    "%pip install python-dotenv\n",
    "%pip install --quiet langchain openai langchain-openai unstructured\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e080661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create directories based on title and source\n",
    "def create_directory_structure(base_dir, title, source):\n",
    "    dir_path = os.path.join(base_dir, title, source)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data to a file with structured naming\n",
    "def save_data(data, dir_path, title, source):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    file_name = f\"{title}_{source}_{timestamp}.txt\"\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(data)\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb285f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to scrape and save data\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import wikipediaapi\n",
    "\n",
    "def scrape_and_save(title, url=None):\n",
    "    base_dir = \"data/scraped_data\"\n",
    "    if url:\n",
    "        source = url.split(\"//\")[-1].split(\"/\")[0]\n",
    "        data = scrape_general(url)\n",
    "    else:\n",
    "        source = \"wikipedia\"\n",
    "        data = scrape_wikipedia(title)\n",
    "    \n",
    "    dir_path = create_directory_structure(base_dir, title, source)\n",
    "    file_path = save_data(data, dir_path, title, source)\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff11ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### For scraping and saving data to scraped_data but not efficient scrapping ############################\n",
    "title = \"Interstellar_(film)\"\n",
    "wikipedia_url = \"https://en.wikipedia.org/wiki/Interstellar_(film)\"\n",
    "# fandom_url = \"https://interstellarfilm.fandom.com/wiki/Interstellar_Wiki\"\n",
    "\n",
    "# Scrape and save data from Wikipedia\n",
    "wiki_file_path = scrape_and_save(title)\n",
    "print(f\"Data saved to: {wiki_file_path}\")\n",
    "\n",
    "# # Scrape and save data from Fandom\n",
    "# fandom_file_path = scrape_and_save(title, fandom_url)\n",
    "# print(f\"Data saved to: {fandom_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed127075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def scrape_wikipedia_content(url, depth=0, max_depth=1, iteration=1, max_iterations=20, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()  # Initialize the set of visited URLs\n",
    "    \n",
    "    if depth > max_depth or iteration > max_iterations:\n",
    "        print(f\"Stopping recursion at depth {depth} and iteration {iteration}\")\n",
    "        return  # Stop recursion based on depth and iteration limits\n",
    "\n",
    "    if url in visited:\n",
    "        print(f\"Already visited {url}\")\n",
    "        return  # Avoid re-scraping the same URL\n",
    "    visited.add(url)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.find(id='firstHeading').text.replace('/', '_')  # Sanitize title for file path\n",
    "    dir_path = os.path.join('data/rough_scraped_data', title)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    content = soup.find(id='mw-content-text')\n",
    "    if content:\n",
    "        content_text = content.get_text()\n",
    "        file_path = os.path.join(dir_path, f\"{title}_Wikipedia_chunk{iteration}.txt\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(content_text)\n",
    "        print(f\"Saved content to {file_path}\")\n",
    "\n",
    "    # Recurse only if under max iterations\n",
    "    if iteration < max_iterations:\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and not ':' in href:\n",
    "                full_url = f'https://en.wikipedia.org{href}'\n",
    "                if full_url not in visited:\n",
    "                    print(f\"Recursing into {full_url} at iteration {iteration+1}\")\n",
    "                    scrape_wikipedia_content(full_url, depth + 1, max_depth, iteration + 1, max_iterations, visited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ddb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "scrape_wikipedia_content('https://en.wikipedia.org/wiki/Interstellar_(film)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################Scrape further links for Fetched Entities of Chatacters like acotes, director etc ##################\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def read_all_json_files(directory_path):\n",
    "    \"\"\"Read all JSON files in the given directory and extract unique character names and aliases.\"\"\"\n",
    "    names = set()\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            for item in data:\n",
    "                names.add(item['character'])\n",
    "                names.update(item.get('aliases', []))\n",
    "    return names\n",
    "\n",
    "def find_relevant_links(soup, names):\n",
    "    \"\"\"Find and return links that are relevant based on the names provided.\"\"\"\n",
    "    relevant_links = []\n",
    "    for link in soup.find_all('a', href=True, title=True):\n",
    "        if any(name in link['title'] for name in names):\n",
    "            relevant_links.append(link['href'])\n",
    "    return relevant_links\n",
    "\n",
    "def scrape_relevant_content(start_url, root_json_dir):\n",
    "    \"\"\"Scrape content from Wikipedia based on character names extracted from JSON files in a specific directory.\"\"\"\n",
    "    title = unquote(start_url.split('/')[-1]).replace('_', ' ')\n",
    "    json_dir_path = os.path.join(root_json_dir, title)\n",
    "    \n",
    "    if not os.path.exists(json_dir_path):\n",
    "        \n",
    "        print(f\"No JSON directory found for title: {title}\")\n",
    "        return\n",
    "    \n",
    "    names = read_all_json_files(json_dir_path)\n",
    "    response = requests.get(start_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    relevant_links = find_relevant_links(soup, names)\n",
    "    base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "    for href in relevant_links:\n",
    "        full_url = base_url + href\n",
    "        response = requests.get(full_url)\n",
    "        page_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_title = unquote(href.split('/')[-1]).replace('_', ' ')\n",
    "        dir_path = os.path.join('data/targeted_scraped_data', page_title)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        file_path = os.path.join(dir_path, f\"{page_title}.txt\")\n",
    "\n",
    "        # Extract and save only the textual content from the article\n",
    "        text_content = page_soup.find('div', id='mw-content-text').get_text(separator='\\n', strip=True)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text_content)\n",
    "        print(f\"Saved content related to {page_title} to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "scrape_relevant_content('https://en.wikipedia.org/wiki/Breaking_Bad', 'data/Entity_json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4145b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Updated tool for Scrapping ############################\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "def parse_page(html, base_url):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Extract the title for the directory name\n",
    "    title_tag = soup.find('h1')\n",
    "    title_text = title_tag.text.replace('/', '_')  # Replace '/' in titles with '_' to avoid path issues\n",
    "    \n",
    "    # Extract all text from the main content\n",
    "    content = soup.find('div', {'id': 'mw-content-text'})\n",
    "    text = content.get_text() if content else \"\"\n",
    "    \n",
    "    # Find links within the content that include the main title\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True) if a.text and title_text in a.text]\n",
    "    \n",
    "    # Convert relative links to absolute\n",
    "    links = ['https://en.wikipedia.org' + link if link.startswith('/wiki/') else link for link in links]\n",
    "    \n",
    "    return text, links, title_text\n",
    "    \n",
    "\n",
    "def scrape_wikipedia(start_url):\n",
    "    html = fetch_page(start_url)\n",
    "    text, links, title = parse_page(html, start_url)\n",
    "    \n",
    "    # For simplicity, just fetch text from the first few relevant links\n",
    "    for link in links[:10]:  # Limit to first 10 links to avoid too many requests\n",
    "        html = fetch_page(link)\n",
    "        page_text, _ , _ = parse_page(html, link)  # We don't follow further links here\n",
    "        text += \"\\n\\n\" + page_text\n",
    "\n",
    "    base_dir = os.path.join('data/scraped_data', title)\n",
    "    os.makedirs(base_dir, exist_ok=True)  # Create directory if it does not exist\n",
    "    \n",
    "    # Format current datetime for the filename\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    filename = f\"{title}_{now}_Wikipedia.txt\"\n",
    "    filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    # Save the content to a text file\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "    return text, title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c15746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "url = 'https://en.wikipedia.org/wiki/Scent_of_a_Woman_(1992_film)'\n",
    "collected_text, title = scrape_wikipedia(url)\n",
    "print(len(collected_text))\n",
    "print(f\"Data collected for {title} and saved to the respective directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86915ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ATIFHANIF/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cell to import necessary libraries\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "nltk.download('stopwords')\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_runnable,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### For cleaning scrapped Data and dividing it into chunks on the basis of tokenization ############################\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Normalize whitespace and remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "def chunk_text(text, chunk_size=8000):\n",
    "    words = word_tokenize(text)\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    # Handle the last chunk; merge with previous if too small\n",
    "    if len(chunks) > 1 and len(word_tokenize(chunks[-1])) < 1000:\n",
    "        chunks[-2] += ' ' + chunks.pop()\n",
    "    return chunks\n",
    "\n",
    "def save_chunks(chunks, base_dir, filename):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_filename = f\"{filename}_chunk{i+1}.txt\"\n",
    "        chunk_path = os.path.join(base_dir, chunk_filename)\n",
    "        with open(chunk_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(chunk)\n",
    "\n",
    "def clean_data(scraped_dir, cleaned_dir):\n",
    "    for title in os.listdir(scraped_dir):\n",
    "        title_path = os.path.join(scraped_dir, title)\n",
    "        cleaned_title_path = os.path.join(cleaned_dir, title)\n",
    "        \n",
    "        if not os.path.exists(cleaned_title_path):\n",
    "            os.makedirs(cleaned_title_path, exist_ok=True)\n",
    "        \n",
    "        for file in os.listdir(title_path):\n",
    "            file_path = os.path.join(title_path, file)\n",
    "            cleaned_file_path = os.path.join(cleaned_title_path, file.replace('.txt', ''))\n",
    "            \n",
    "            # Check if already cleaned\n",
    "            if not any(f.startswith(file.replace('.txt', '')) for f in os.listdir(cleaned_title_path)):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                # Clean the text\n",
    "                cleaned_text = normalize_text(text)\n",
    "                chunks = chunk_text(cleaned_text)\n",
    "                \n",
    "                # Save cleaned chunks\n",
    "                save_chunks(chunks, cleaned_title_path, file.replace('.txt', ''))\n",
    "                print(f\"Data cleaned and saved for {file_path}\")\n",
    "            else:\n",
    "                print(f\"Already cleaned data present for {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31505b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "scraped_dir = 'data/scraped_data'\n",
    "cleaned_dir = 'data/cleaned_data'\n",
    "clean_data(scraped_dir, cleaned_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7408ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to define the text normalization function\n",
    "def normalize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0280f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to define the function to remove unnecessary characters\n",
    "def remove_unnecessary_characters(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to define the context-based cleaning function using LangChain and OpenAI API\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "import tiktoken\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0,)\n",
    "\n",
    "def context_based_cleaning(input_text: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text based on the given context using ChatOpenAI.\n",
    "\n",
    "    :param input_text: The text to be cleaned.\n",
    "    :param context: The context to guide the cleaning process.\n",
    "    :return: The cleaned text.\n",
    "    \"\"\"\n",
    "    def split_text(text, max_tokens):\n",
    "        # Tokenize the text using tiktoken\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        \n",
    "        # Split the tokens into chunks of max_tokens\n",
    "        chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "        \n",
    "        # Decode the token chunks back to text\n",
    "        text_chunks = [enc.decode(chunk) for chunk in chunks]\n",
    "        \n",
    "        return text_chunks\n",
    "\n",
    "    max_tokens_per_chunk = 5000 - 385  # Leaving room for prompt tokens\n",
    "\n",
    "    # Split the input text into manageable chunks\n",
    "    text_chunks = split_text(input_text, max_tokens_per_chunk)\n",
    "\n",
    "    cleaned_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a text cleaning assistant. Your task is to clean the input text based on the provided context.\n",
    "    ## Context\n",
    "    {context}\n",
    "    ## Instructions\n",
    "    - Remove irrelevant information.\n",
    "    - Correct grammatical errors.\n",
    "    - Ensure the text is clear and concise.\n",
    "    - Maintain the original meaning as much as possible.\n",
    "    ## Output\n",
    "    Provide the cleaned version of the text in the same language as the input.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Clean the following text based on the context:\\n\\n{chunk}\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = llm.invoke(messages)\n",
    "        cleaned_chunks.append(response.content)\n",
    "\n",
    "    # Concatenate the cleaned chunks to get the full cleaned text\n",
    "    cleaned_text = \"\\n\".join(cleaned_chunks)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# input_text = \"Ths is a smple textt with somee speling mistkes and extraneous infformation dsfsdda44 54454 sfdfad.;;';.\"\n",
    "# context = \"This text is a part of a formal document and should be cleaned accordingly.\"\n",
    "\n",
    "# cleaned_text = clean_text(input_text, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to define the main cleaning function\n",
    "def clean_data(text):\n",
    "    context = \"This text is a part of a formal document and should be cleaned accordingly.You are a helpful assistant. Clean and standardize the following text to make it more readable and consistent.\"\n",
    "    text = normalize_text(text)\n",
    "    text = remove_unnecessary_characters(text)\n",
    "    # text = context_based_cleaning(text,context)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to create directories for cleaned data based on title and source\n",
    "def create_cleaned_directory_structure(base_dir, title, source):\n",
    "    dir_path = os.path.join(base_dir, title, source)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to clean the scraped data and save it in the cleaned_data directory\n",
    "def clean_and_save_scraped_data(scraped_dir, cleaned_dir):\n",
    "    for root, dirs, files in os.walk(scraped_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = f.read()\n",
    "                \n",
    "                cleaned_data = clean_data(data)\n",
    "                \n",
    "                # Extract title and source from the file path\n",
    "                relative_path = os.path.relpath(file_path, scraped_dir)\n",
    "                title, source = relative_path.split(os.sep)[:2]\n",
    "                \n",
    "                cleaned_dir_path = create_cleaned_directory_structure(cleaned_dir, title, source)\n",
    "                cleaned_file_path = os.path.join(cleaned_dir_path, file)\n",
    "                \n",
    "                with open(cleaned_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(cleaned_data)\n",
    "                \n",
    "                print(f\"Cleaned data saved to: {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a36ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scraped_dir = \"data/scraped_data\"\n",
    "cleaned_dir = \"data/cleaned_data\"\n",
    "\n",
    "clean_and_save_scraped_data(scraped_dir, cleaned_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48fee39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "import tiktoken\n",
    "# print(api)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d0793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### For Non Chunked Data ############################\n",
    "\n",
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "import tiktoken\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "\n",
    "def read_files_from_directory(directory_path: str):\n",
    "    \"\"\"\n",
    "    Read text files from the specified directory.\n",
    "\n",
    "    :param directory_path: The path to the directory containing text files.\n",
    "    :return: A dictionary with file names as keys and file contents as values.\n",
    "    \"\"\"\n",
    "    files_content = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    files_content[file_path] = file.read()\n",
    "    print(f\"Read {len(files_content)} files from directory {directory_path}\")\n",
    "    return files_content\n",
    "\n",
    "def split_text(text, max_tokens):\n",
    "    \"\"\"\n",
    "    Split text into smaller chunks based on token limits.\n",
    "\n",
    "    :param text: The text to be split.\n",
    "    :param max_tokens: The maximum number of tokens per chunk.\n",
    "    :return: A list of text chunks.\n",
    "    \"\"\"\n",
    "    enc = tiktoken.get_encoding('gpt2')\n",
    "    tokens = enc.encode(text)\n",
    "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    text_chunks = [enc.decode(chunk) for chunk in chunks]\n",
    "    return text_chunks\n",
    "\n",
    "def extract_information(text: str, title: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract information from the text using the LLM.\n",
    "\n",
    "    :param text: The text to be processed.\n",
    "    :param title: The title of the TV show.\n",
    "    :return: A JSON object with extracted information.\n",
    "    \"\"\"\n",
    "    max_tokens_per_chunk = 5000 - 385  # Leave room for prompt tokens\n",
    "    text_chunks = split_text(text, max_tokens_per_chunk)\n",
    "\n",
    "    combined_results = []\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"I want you to imagine you are reading a text page on the TV show {title} for the first time and have no prior knowledge of TV show {title}.\n",
    "After reading that text -- I want you to extract a list of people that you can identify from that piece of text. I also want you to give me the smallest fragment of text where you identified that person in the given text. Note there may be multiple references to a person you identify and I want you to list all of them. I also want you to give me potential aliases for each person.\n",
    "Return your answers in a JSON object. The returned JSON object should be a list of references objects. Where the reference objects are discretionary of the person and text fragments where the person was identified.\n",
    "\n",
    "An example of a valid response fragment is as follows:\n",
    "\n",
    "[\n",
    "    {{\n",
    "        \"person\": \"Walter White\",\n",
    "        \"aliases\": [\n",
    "            \"Walt\",\n",
    "            \"Walter\"\n",
    "       ]\n",
    "        \"fragments\": [\n",
    "            \"Walter White settles into his new surroundings and takes a liking to his new lab assistant Gale Boetticher.\",\n",
    "            \"Marie Schrader, who suggests that he ask Walt about it, due to Walt's previous association with Jesse.\",\n",
    "            \"He tells them again that he will not allow them to kill Walt until his business with him has concluded, but gives them his blessing to instead go after the man who actually pulled the trigger on Tuco Salamanca: Hank.\"\n",
    "        ]\n",
    "    }},\n",
    "    {{\n",
    "        \"person\": \"Gale Boetticher\",\n",
    "       \"aliases\": [\"Gale\"]\n",
    "        \"fragments\": [\n",
    "            \"Walter White settles into his new surroundings and takes a liking to his new lab assistant Gale Boetticher.\"\n",
    "        ]\n",
    "    }}\n",
    "]\n",
    "Below is part of that wikipedia page on the TV show {title}.\n",
    "\n",
    "\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Below is part of that Wikipedia page on the TV show {title}.\\n\\n{chunk}\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        chunk_results = json.loads(response.content)\n",
    "        combined_results.extend(chunk_results)\n",
    "\n",
    "    # Combine results by merging fragments and aliases for the same person\n",
    "    merged_results = {}\n",
    "    for item in combined_results:\n",
    "        person = item['person']\n",
    "        if person not in merged_results:\n",
    "            merged_results[person] = {\n",
    "                'person': person,\n",
    "                'aliases': set(item['aliases']),\n",
    "                'fragments': set(item['fragments'])\n",
    "            }\n",
    "        else:\n",
    "            merged_results[person]['aliases'].update(item['aliases'])\n",
    "            merged_results[person]['fragments'].update(item['fragments'])\n",
    "\n",
    "    # Convert sets back to lists\n",
    "    for person in merged_results:\n",
    "        merged_results[person]['aliases'] = list(merged_results[person]['aliases'])\n",
    "        merged_results[person]['fragments'] = list(merged_results[person]['fragments'])\n",
    "\n",
    "    return list(merged_results.values())\n",
    "\n",
    "def save_json_to_file(data: dict, file_path: str):\n",
    "    \"\"\"\n",
    "    Save the JSON data to a file.\n",
    "\n",
    "    :param data: The data to be saved.\n",
    "    :param file_path: The path to the file where the data will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def process_cleaned_data(directory_path: str, title: str = None, is_file: bool = False):\n",
    "    \"\"\"\n",
    "    Process all cleaned data files and extract information.\n",
    "\n",
    "    :param directory_path: The path to the directory containing cleaned text files or the path to a specific file.\n",
    "    :param title: The title of the TV show (if known).\n",
    "    :param is_file: A boolean indicating whether the path is a file.\n",
    "    \"\"\"\n",
    "    if is_file:\n",
    "        # Process a single file\n",
    "        with open(directory_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        title = title or os.path.basename(directory_path).split('.')[0]\n",
    "        extracted_info = extract_information(content, title)\n",
    "        json_filename = os.path.basename(directory_path).replace(\".txt\", \".json\")\n",
    "        json_filepath = os.path.join(\"data/Entity_json\", json_filename)\n",
    "        save_json_to_file(extracted_info, json_filepath)\n",
    "        print(f\"Processed and saved: {json_filepath}\")\n",
    "    else:\n",
    "        # Process a directory\n",
    "        files_content = read_files_from_directory(directory_path)\n",
    "        if not files_content:\n",
    "            print(\"No files found in the directory.\")\n",
    "            return\n",
    "        for filepath, content in files_content.items():\n",
    "            # Derive title from directory structure if not provided\n",
    "            relative_path = os.path.relpath(filepath, directory_path)\n",
    "            path_parts = relative_path.split(os.sep)\n",
    "            title = path_parts[0]  # Assuming title is the first part of the path\n",
    "            source = path_parts[1]  # Assuming source is the second part of the path\n",
    "            extracted_info = extract_information(content, title)\n",
    "            json_filename = os.path.basename(filepath).replace(\".txt\", \".json\")\n",
    "            json_filepath = os.path.join(\"data/Entity_json\", title, source, json_filename)\n",
    "            save_json_to_file(extracted_info, json_filepath)\n",
    "            print(f\"Processed and saved: {json_filepath}\")\n",
    "\n",
    "# Example usage\n",
    "# Process a single file\n",
    "# process_cleaned_data(\"cleaned_data/Breaking_Bad/wikipedia/sample.txt\", title=\"Breaking Bad\", is_file=True)\n",
    "\n",
    "# Process a directory\n",
    "process_cleaned_data(\"data/cleaned_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### For Chunked Data ############################\n",
    "\n",
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize ChatOpenAI with the model and API key\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "def read_files_from_directory(directory_path: str):\n",
    "    \"\"\"\n",
    "    Read text files from the specified directory.\n",
    "    \"\"\"\n",
    "    files_content = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    files_content[file_path] = file.read()\n",
    "    print(f\"Read {len(files_content)} files from directory {directory_path}\")\n",
    "    return files_content\n",
    "\n",
    "\n",
    "def extract_information(llm, text: str, title: str) -> dict:\n",
    "    prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "        I am analyzing the TV show '{title}' and need to extract detailed information about characters to understand plot dynamics better. Please list all identifiable characters from the text below. For each character, provide:\n",
    "        - A short fragment of text that captures their essence or a significant action.\n",
    "        - Potential aliases.\n",
    "        Fragments should not mention lists of character names from the TV show or movie. Provide the smallest fragment with contextual meaning.\n",
    "        The response should be concise and structured in JSON format to facilitate relationship analysis in future iterations.\n",
    "\n",
    "        An example of a valid response:\n",
    "        [\n",
    "            {{\n",
    "                \"character\": \"Jon Snow\",\n",
    "                \"aliases\": [\"Lord Snow\", \"The White Wolf\"],\n",
    "                \"fragment\": \"Jon Snow pledges his life to the Night's Watch and refuses to leave even when tempted.\"\n",
    "            }},\n",
    "            {{\n",
    "                \"character\": \"Daenerys Targaryen\",\n",
    "                \"aliases\": [\"Dany\", \"Khaleesi\"],\n",
    "                \"fragment\": \"Daenerys sets sail for Westeros with her armies and dragons, aiming to reclaim her family's throne.\"\n",
    "            }}\n",
    "        ]\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    user_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Below is part of that Wikipedia page on the TV show {title}.\\n\\n{text}\"\n",
    "    }\n",
    "\n",
    "    combined_prompts = [prompt, user_prompt]  # This should be a list of dictionaries\n",
    "\n",
    "    response = llm.invoke(combined_prompts)\n",
    "    try:\n",
    "        response_json = json.loads(response.content)\n",
    "        return response_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        print(f\"Received malformed JSON response: {response.content}\")\n",
    "        return []  # Return an empty list to indicate failure\n",
    "\n",
    "\n",
    "def save_json_to_file(data: dict, file_path: str):\n",
    "    \"\"\"\n",
    "    Save the JSON data to a file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# def process_cleaned_data(directory_path: str, title: str = None, is_file: bool = False):\n",
    "#     \"\"\"\n",
    "#     Process all cleaned data files and extract information.\n",
    "#     \"\"\"\n",
    "#     isProcess =True\n",
    "#     if is_file:\n",
    "#         title = title or os.path.basename(directory_path).split('.')[0]\n",
    "#         entity_json_path = f\"data/Entity_json/{title}.json\"\n",
    "#         if os.path.exists(entity_json_path):\n",
    "#             user_input = input(f\"The title '{title}' is already in Entity_json. Would you like to process it again? (yes/no): \")\n",
    "#             if user_input.lower() != 'yes':\n",
    "#                 print(f\"Skipping processing for {title}.\")\n",
    "#                 return\n",
    "#         content = read_files_from_directory(directory_path)\n",
    "#         extracted_info = extract_information(llm, content, title)\n",
    "#         save_json_to_file(extracted_info, entity_json_path)\n",
    "#         print(f\"Processed and saved: {entity_json_path}\")\n",
    "#     else:\n",
    "#         # Process a directory\n",
    "#         files_content = read_files_from_directory(directory_path)\n",
    "#         if not files_content:\n",
    "#             print(\"No files found in the directory.\")\n",
    "#             return\n",
    "#         for filepath, content in files_content.items():\n",
    "#             title = os.path.basename(filepath).split('_')[0]  # Derive title from filename\n",
    "#             print(title)\n",
    "#             entity_json_path = f\"data/Entity_json/{title}\"\n",
    "#             print(entity_json_path)\n",
    "#             if os.path.exists(entity_json_path):\n",
    "                \n",
    "#                 user_input = input(f\"The title '{title}' is already in Entity_json. Would you like to process it again? (yes/no): \")\n",
    "#                 if user_input.lower() != 'yes':\n",
    "#                     print(f\"Skipping processing for {title}.\")\n",
    "#                     continue\n",
    "                 \n",
    "#             # extracted_info = extract_information(llm, content, title)\n",
    "#             # save_json_to_file(extracted_info, entity_json_path)\n",
    "#             print(f\"Processed and saved: {entity_json_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_cleaned_data(cleaned_data_dir: str):\n",
    "    \"\"\"\n",
    "    Process all cleaned data directories and extract information.\n",
    "    \"\"\"\n",
    "    for title in os.listdir(cleaned_data_dir):\n",
    "        title_dir_path = os.path.join(cleaned_data_dir, title)\n",
    "        entity_json_dir_path = os.path.join(\"data/Entity_json\", title)\n",
    "        \n",
    "        if os.path.exists(entity_json_dir_path):\n",
    "            user_input = input(f\"The title '{title}' is already in Entity_json. Would you like to process it again? (yes/no): \")\n",
    "            if user_input.lower() != 'yes':\n",
    "                print(f\"Skipping processing for all files under the title '{title}'.\")\n",
    "                continue\n",
    "            else:\n",
    "                # Remove existing JSON files to replace with new ones\n",
    "                for file in os.listdir(entity_json_dir_path):\n",
    "                    if file.endswith('.json'):\n",
    "                        os.remove(os.path.join(entity_json_dir_path, file))\n",
    "\n",
    "        files_content = read_files_from_directory(title_dir_path)\n",
    "        for filepath, content in files_content.items():\n",
    "            extracted_info = extract_information(llm, content, title)\n",
    "            json_filename = os.path.basename(filepath).replace(\".txt\", \".json\")\n",
    "            json_filepath = os.path.join(entity_json_dir_path, json_filename)\n",
    "            save_json_to_file(extracted_info, json_filepath)\n",
    "            print(f\"Processed and saved: {json_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cbe997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# Process a directory\n",
    "process_cleaned_data(\"data/cleaned_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_narrative_elements(llm, text: str, title: str) -> dict:\n",
    "    prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "        I am analyzing the TV show '{title}' for the first time and need to extract detailed information about key narrative elements such as objects, locations, addresses, scenes, and events. Please identify these elements from the text below and provide:\n",
    "        - A concise description or identification of the element.\n",
    "        - A list of excerpts that discuss this element, ensuring each excerpt provides meaningful context.\n",
    "\n",
    "        The response should be concise and structured in JSON format to facilitate analysis and visualization of these narrative elements in future iterations.\n",
    "\n",
    "        An example of a valid response:\n",
    "        [\n",
    "            {{\n",
    "                \"element\": \"Shooting scene\",\n",
    "                \"description\": \"Episode 3, Season 4\",\n",
    "                \"excerpts\": [\n",
    "                    \"The intense shooting scene in the third episode of the fourth season was pivotal to the plot development.\",\n",
    "                    \"During the shootout, the main character's dilemma comes to a head, forcing a decision that changes the course of the story.\"\n",
    "                ]\n",
    "            }},\n",
    "            {{\n",
    "                \"element\": \"Central Park\",\n",
    "                \"description\": \"Location\",\n",
    "                \"excerpts\": [\n",
    "                    \"Several key discussions between the protagonists occur in Central Park, serving as a backdrop to their evolving relationships.\",\n",
    "                    \"Central Park is depicted in multiple scenes as a place of reflection and confrontation among the characters.\"\n",
    "                ]\n",
    "            }}\n",
    "        ]\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    user_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Below is part of that Wikipedia page on the TV show {title}.\\n\\n{text}\"\n",
    "    }\n",
    "\n",
    "    combined_prompts = [prompt, user_prompt]  # This should be a list of dictionaries\n",
    "\n",
    "    response = llm.invoke(combined_prompts)\n",
    "    try:\n",
    "        response_json = json.loads(response.content)\n",
    "        return response_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        print(f\"Received malformed JSON response: {response.content}\")\n",
    "        return []  # Return an empty list to indicate failure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_cleaned_data_Events(directory_path: str, title: str = None, is_file: bool = False):\n",
    "    \"\"\"Process all cleaned data files and extract narrative information.\"\"\"\n",
    "    if is_file:\n",
    "        with open(directory_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        title = title or os.path.basename(directory_path).split('.')[0]\n",
    "        extracted_info = extract_narrative_elements(llm, content, title)\n",
    "        json_filename = os.path.basename(directory_path).replace(\".txt\", \".json\")\n",
    "        json_filepath = os.path.join(\"data/Events_json\", json_filename)\n",
    "        save_json_to_file(extracted_info, json_filepath)\n",
    "        print(f\"Processed and saved: {json_filepath}\")\n",
    "    else:\n",
    "        files_content = read_files_from_directory(directory_path)\n",
    "        if not files_content:\n",
    "            print(\"No files found in the directory.\")\n",
    "            return\n",
    "        for filepath, content in files_content.items():\n",
    "            title = os.path.basename(filepath).split('_')[0]  # Derive title from filename\n",
    "            extracted_info = extract_narrative_elements(llm, content, title)\n",
    "            json_filename = os.path.basename(filepath).replace(\".txt\", \".json\")\n",
    "            json_filepath = os.path.join(\"data/Events_json\", title, json_filename)\n",
    "            save_json_to_file(extracted_info, json_filepath)\n",
    "            print(f\"Processed and saved: {json_filepath}\")\n",
    "\n",
    "process_cleaned_data_Events(\"data/cleaned_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b915856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the language model\n",
    "\n",
    "def merge_json_files(directory_path):\n",
    "    \"\"\"Merge all JSON files in a directory into a single JSON list.\"\"\"\n",
    "    merged_data = []\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"No directory found at {directory_path}\")\n",
    "        return merged_data\n",
    "\n",
    "    for file in os.listdir(directory_path):\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(directory_path, file), 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    if data:  # Ensure that data is not empty\n",
    "                        merged_data.extend(data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from file {file}: {e}\")\n",
    "    return merged_data\n",
    "\n",
    "def process_title_entities(title_directory):\n",
    "    \"\"\"Process all entity JSON files for a given title.\"\"\"\n",
    "    merged_data = merge_json_files(title_directory)\n",
    "    if not merged_data:\n",
    "        print(\"No data found to merge.\")\n",
    "        return\n",
    "\n",
    "    title = os.path.basename(title_directory)\n",
    "    # Save the merged data\n",
    "    merged_dir = os.path.join('data/Merged_Entity_json', title)\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    merged_file_path = os.path.join(merged_dir, f\"{title}_Merged.json\")\n",
    "    with open(merged_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, indent=4)\n",
    "    print(f\"Merged data saved to {merged_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "title_directory = 'data/Entity_json/Interstellar (film)'  # Specify the subfolder for a specific title\n",
    "process_title_entities(title_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### For Deduping with LLM (have ambiguities look for beeter approch below)##############################\n",
    "\n",
    "import json\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "# Initialize the language model\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_data(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def normalize_name(name):\n",
    "    return ' '.join(name.lower().strip().split())\n",
    "\n",
    "def deduplicate_with_llm(entities):\n",
    "    \"\"\"Use LLM to semantically deduplicate entities based on descriptions and context.\"\"\"\n",
    "    deduplicated = []\n",
    "    used = set()  # Track indices of entities that have been merged\n",
    "\n",
    "    for i, current in enumerate(entities):\n",
    "        if i in used:\n",
    "            continue\n",
    "\n",
    "        # Normalize and set up initial entity structure\n",
    "        current['aliases'] = set([normalize_name(alias) for alias in current.get('aliases', [])] + [normalize_name(current['character'])])\n",
    "        current['fragments'] = set([current['fragment']])\n",
    "\n",
    "        for j in range(i + 1, len(entities)):\n",
    "            if j in used:\n",
    "                continue\n",
    "            other = entities[j]\n",
    "\n",
    "            # Normalize other entity data\n",
    "            other['aliases'] = set([normalize_name(alias) for alias in other.get('aliases', [])] + [normalize_name(other['character'])])\n",
    "            \n",
    "            # Construct the prompt to check if entities are the same\n",
    "            prompt = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"Consider these entities:\\n1. {current['character']} with aliases {list(current['aliases'])} and fragment: {current['fragment']}\\n2. {other['character']} with aliases {list(other['aliases'])} and fragment: {other['fragment']}\\nAre these descriptions of the same character?\"\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            response = llm.invoke(prompt)  # Invoke the LLM with the prompt\n",
    "            response_text = response.content  # Access the response content directly\n",
    "\n",
    "            # Check if the response indicates the entities are the same\n",
    "            if 'yes' in response_text.lower():\n",
    "                current['aliases'].update(other['aliases'])\n",
    "                current['fragments'].update([other['fragment']])\n",
    "                used.add(j)\n",
    "\n",
    "        # Convert sets back to lists for JSON serialization\n",
    "        current['aliases'] = list(current['aliases'])\n",
    "        current['fragments'] = list(current['fragments'])\n",
    "        deduplicated.append(current)\n",
    "\n",
    "    return deduplicated\n",
    "\n",
    "# Example use of the function remains the same\n",
    "# def deduplicate_with_llm(entities):\n",
    "#     \"\"\" Use LLM to semantically deduplicate entities based on descriptions and context. \"\"\"\n",
    "#     deduplicated = []\n",
    "#     used = set()  # Track indices of entities that have been merged\n",
    "\n",
    "#     for i, current in enumerate(entities):\n",
    "#         if i in used:\n",
    "#             continue\n",
    "\n",
    "#         current['aliases'] = set([normalize_name(alias) for alias in current.get('aliases', [])] + [normalize_name(current['character'])])\n",
    "#         current['fragment'] = set(current.get('fragment', [current['fragment']]))\n",
    "\n",
    "#         for j in range(i + 1, len(entities)):\n",
    "#             if j in used:\n",
    "#                 continue\n",
    "#             other = entities[j]\n",
    "\n",
    "#             other['aliases'] = set([normalize_name(alias) for alias in other.get('aliases', [])] + [normalize_name(other['character'])])\n",
    "#             other['fragment'] = set(other.get('fragment', [other['fragment']]))\n",
    "\n",
    "#             prompt = [\n",
    "#                 {\n",
    "#                     \"role\": \"system\",\n",
    "#                     \"content\": f\"\"\"\n",
    "#                     Please analyze the following character profiles:\n",
    "\n",
    "#                     Profile 1:\n",
    "#                     Name: {current['character']}\n",
    "#                     Aliases: {', '.join(current['aliases'])}\n",
    "#                     Key Fragments: {', '.join(current['fragment'])}\n",
    "\n",
    "#                     Profile 2:\n",
    "#                     Name: {other['character']}\n",
    "#                     Aliases: {', '.join(other['aliases'])}\n",
    "#                     Key Fragments: {', '.join(other['fragment'])}\n",
    "\n",
    "#                     Considering their names, aliases, and the contexts provided by the key fragments, are these profiles describing the same individual? Evaluate their identities based on overlapping information and narrative connections.\n",
    "#                     \"\"\"\n",
    "#                 }\n",
    "#             ]\n",
    "\n",
    "#             response = llm.invoke(prompt)  # Invoke the LLM with the prompt\n",
    "#             response_text = response.content  # Access the response content directly\n",
    "#             print(response_text)\n",
    "#             # Check if the response indicates the entities are the same\n",
    "#             if 'yes' in response_text.lower() or 'likely' in response_text.lower():\n",
    "#                 current['aliases'].update(other['aliases'])\n",
    "#                 current['fragment'].update(other['fragment'])\n",
    "#                 used.add(j)\n",
    "\n",
    "#         # Convert sets back to lists for JSON serialization\n",
    "#         current['aliases'] = list(current['aliases'])\n",
    "#         current['fragment'] = list(current['fragment'])\n",
    "#         deduplicated.append(current)\n",
    "\n",
    "#     return deduplicated\n",
    "# Load data\n",
    "entities = load_data('data/Merged_Entity_json/Interstellar (film)/Interstellar (film)_Merged.json')\n",
    "\n",
    "# Deduplicate entities\n",
    "deduplicated_entities = deduplicate_with_llm(entities)\n",
    "\n",
    "# Save deduplicated data\n",
    "save_data(deduplicated_entities, 'data/Deduped_Entity_json/Interstellar/Interstellar_Deduped.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fad0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### For Deduping without LLM ##############################\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def write_json(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def merge_entries(entries):\n",
    "    # Dictionary to hold merged characters\n",
    "    merged_characters = {}\n",
    "\n",
    "    for entry in entries:\n",
    "        # Identify character key\n",
    "        character_key = entry['character'].lower()\n",
    "        \n",
    "        # If the character is already added, merge the data\n",
    "        if character_key in merged_characters:\n",
    "            existing_entry = merged_characters[character_key]\n",
    "            existing_entry['aliases'] = list(set(existing_entry['aliases'] + entry['aliases']))\n",
    "            existing_entry['fragment'].append(entry['fragment'])\n",
    "        else:\n",
    "            # Otherwise, add new entry\n",
    "            merged_characters[character_key] = {\n",
    "                'character': entry['character'],\n",
    "                'aliases': entry['aliases'],\n",
    "                'fragment': [entry['fragment']]\n",
    "            }\n",
    "\n",
    "    # Convert merged data to list\n",
    "    return list(merged_characters.values())\n",
    "\n",
    "def process_json_file(input_file_path, output_file_path):\n",
    "    # Read data from the input JSON file\n",
    "    entries = read_json(input_file_path)\n",
    "    \n",
    "    # Merge entries\n",
    "    merged_data = merge_entries(entries)\n",
    "    \n",
    "    # Write merged data to the output JSON file\n",
    "    write_json(merged_data, output_file_path)\n",
    "\n",
    "# Usage\n",
    "input_file_path = 'data/Merged_Entity_json/Interstellar (film)/Interstellar (film)_Merged.json'\n",
    "output_file_path = 'data/Deduped_Entity_json/Interstellar/Interstellar_Deduped.json'\n",
    "\n",
    "process_json_file(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5720a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### For Deduping with LLM (have ambiguities look for beeter approch below)##############################\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the OpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "def query_llm(current, other, title):\n",
    "    \"\"\"Query the OpenAI model to determine if two character descriptions are the same in the context of a specific title.\"\"\"\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "            Consider these entities from the movie or TV show '{title}':\n",
    "            1. {current['character']} with aliases {list(current['aliases'])} and fragment: {current['fragment']}\n",
    "            2. {other['character']} with aliases {list(other['aliases'])} and fragment: {other['fragment']}\n",
    "            \n",
    "            Are these descriptions of the same character? Please respond with \"yes\" or \"no\".\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke(prompt)  # Invoke the LLM with the prompt\n",
    "        response_text = response.content.strip()  # Access the response content directly\n",
    "        return response_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in querying LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "def merge_entries_with_context(entries, title):\n",
    "    \"\"\"Merge entries based on context using LLM to determine character identity.\"\"\"\n",
    "    merged_data = []\n",
    "\n",
    "    for current in entries:\n",
    "        current_key = current['character'].lower()\n",
    "        found = False\n",
    "\n",
    "        for merged_entry in merged_data:\n",
    "            response = query_llm(current, merged_entry, title)\n",
    "\n",
    "            if response and \"yes\" in response.lower():\n",
    "                # If they are the same, merge aliases and fragments\n",
    "                merged_entry['aliases'] = list(set(merged_entry['aliases'] + current['aliases'] + [current_key]))\n",
    "                merged_entry['fragment'] += current['fragment']\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            merged_data.append(current)\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def write_json(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def process_json_file(input_file_path, output_file_path, title):\n",
    "    # Read data from the input JSON file\n",
    "    entries = read_json(input_file_path)\n",
    "    \n",
    "    # Merge entries with context understanding\n",
    "    merged_data = merge_entries_with_context(entries, title)\n",
    "    \n",
    "    # Write merged data to the output JSON file\n",
    "    write_json(merged_data, output_file_path)\n",
    "\n",
    "# Usage example\n",
    "input_file_path = 'data/Deduped_Entity_json/Interstellar/Interstellar_Deduped.json'\n",
    "output_file_path = 'data/Deduped_Entity_json/Interstellar/Interstellar_Deduped_llm.json'\n",
    "title = 'Interstellar'  # Example title; replace with the relevant movie or TV show title\n",
    "process_json_file(input_file_path, output_file_path, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c9fe337",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'data/Deduped_Entity_json/Interstellar/Interstellar_Deduped_llm.json'\n",
    "output_file_path = 'data/Deduped_Entity_json/Interstellar/Interstellar_Deduped_llm2.json'\n",
    "\n",
    "process_json_file(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1af4f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'function', 'function': {'name': 'echo_json', 'description': 'Echoes the provided JSON object.', 'parameters': {'type': 'object', 'properties': {'json_data': {'description': 'JSON data to echo back', 'type': 'string'}}, 'required': ['json_data']}}}\n",
      "[\n",
      "    {\n",
      "        \"character\": \"Dr. Mann\",\n",
      "        \"aliases\": [\n",
      "            \"Mann\"\n",
      "        ],\n",
      "        \"fragment\": [\n",
      "            \"Dr. Mann, a NASA astronaut, is sent to an icy planet during the Lazarus program.\"\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"character\": \"Mann\",\n",
      "        \"aliases\": [],\n",
      "        \"fragment\": [\n",
      "            \"The comic is a prequel to the film with Mann as the protagonist.\"\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#Echoicing of json object in output fro Debugging purposes\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import ChatCompletionToolParam\n",
    "from openai.types.shared_params import FunctionDefinition\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a function that acts as a tool for the model\n",
    "function = FunctionDefinition({\n",
    "    'name': 'echo_json',\n",
    "    'description': 'Echoes the provided JSON object.',\n",
    "    'parameters': {\n",
    "        'type': 'object',\n",
    "        'properties': {'json_data': {'description': 'JSON data to echo back', 'type': 'string'}},\n",
    "        'required': ['json_data']\n",
    "    }\n",
    "})\n",
    "\n",
    "toolc = ChatCompletionToolParam({'type': 'function', 'function': function})\n",
    "print(toolc)\n",
    "\n",
    "# Sample JSON to echo\n",
    "json_input = '''\n",
    "[\n",
    "    {\n",
    "        \"character\": \"Dr. Mann\",\n",
    "        \"aliases\": [\n",
    "            \"Mann\"\n",
    "        ],\n",
    "        \"fragment\": [\n",
    "            \"Dr. Mann, a NASA astronaut, is sent to an icy planet during the Lazarus program.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"character\": \"Mann\",\n",
    "        \"aliases\": [],\n",
    "        \"fragment\": [\n",
    "            \"The comic is a prequel to the film with Mann as the protagonist.\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "'''\n",
    "\n",
    "# Creating a chat completion request\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Echo back any JSON data provided by the user.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Please echo this JSON data: {json_input}\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-16k\",  # Make sure the model version supports your requirements\n",
    ")\n",
    "\n",
    "\n",
    "# Access and print the response content\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1350cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e56750e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"characters\": [\n",
      "        {\n",
      "            \"character\": \"Matthew McConaughey\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Matthew McConaughey stars in Interstellar as the main protagonist.\",\n",
      "                \"The teaser trailer for Interstellar debuted December 13, 2013 and featured clips related to space exploration accompanied by a voiceover by Matthew McConaughey's character Cooper.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Anne Hathaway\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Anne Hathaway joins the cast of Interstellar alongside Matthew McConaughey.\",\n",
      "                \"When Murph grows up into Jessica Chastain, a key member of Caine's NASA crew and a surrogate for the daughter that the elder Brand lost to the Endurance's mission.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Jessica Chastain\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Jessica Chastain appears in Interstellar as one of the main characters.\",\n",
      "                \"Cooper's farewell to his daughter Murph, who's played by McKenzie Foy as a young girl, is shot very close-in and lit in warm cradling tones.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Christopher Nolan\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Christopher Nolan directs Interstellar, bringing his unique vision to the film.\",\n",
      "                \"Kip Thorne, a renowned physicist, serves as a science advisor for Interstellar.\",\n",
      "                \"Christopher Nolan and McConaughey made their first appearances at San Diego Comic-Con in July 2014 to promote Interstellar.\",\n",
      "                \"I think really space exploration to me has always represented the most hopeful and optimistic endeavor that mankind has ever really engaged with.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Matt Damon\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Matt Damon joins the cast of Interstellar in a supporting role.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Hoyte van Hoytema\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Hoyte van Hoytema serves as the director of photography for Interstellar.\",\n",
      "                \"Wally Pfister, known for his work with Christopher Nolan, is replaced by Hoyte van Hoytema as the director of photography for Interstellar.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Paul Franklin\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Paul Franklin serves as the visual effects supervisor for Interstellar.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Kip Thorne\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Kip Thorne, a renowned physicist, serves as a science advisor for Interstellar.\",\n",
      "                \"Thorne collaborated with Franklin and a team of 30 people at Double Negative.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Hans Zimmer\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Hans Zimmer composes the score for Interstellar, including the iconic theme.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Cooper\",\n",
      "            \"aliases\": [\n",
      "                \"Coop\"\n",
      "            ],\n",
      "            \"fragments\": [\n",
      "                \"Referred to only as Cooper or Coop in the film.\",\n",
      "                \"Cooper proudly identifies himself as an engineer as well as an astronaut and farmer but he has the soul of a goofball poet.\",\n",
      "                \"We've always defined ourselves by the ability to overcome the impossible.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Mann\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Mann was in the hibernation state when cooper arrives.\",\n",
      "                \"Dr. Mann, a NASA astronaut, is sent to an icy planet during the Lazarus program.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Thorne\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Thorne collaborated with Franklin and a team of 30 people at Double Negative.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Murph\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Cooper's farewell to his daughter Murph, who's played by McKenzie Foy as a young girl, is shot very close-in and lit in warm cradling tones.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"TARS\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Snappy comic patter mostly between Cooper and the ship's robot TARS, designed in Minecraft-style pixelish boxes and voiced by Bill Irwin.\",\n",
      "                \"TARS is an intelligent robot assigned to assist the crew of the Endurance.\",\n",
      "                \"Everybody good? Plenty of slaves for my robot colony.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Professor Brand\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Do not go gentle into that good night.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Murphy Cooper\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Today is my birthday and it's a special one because you once told me that when you came back we might be the same age.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Amelia Brand\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Love is the one thing that transcends time and space.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Case\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"TARS talks plenty for both of us.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Joseph Cooper\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Joseph Cooper, a former NASA test pilot, reluctantly becomes a farmer after the agency was closed by the government.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Dr. Amelia Brand\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Dr. Amelia Brand, professor Brand's daughter and NASA scientist, is responsible for conducting planet colonization.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Murphy Murph Cooper\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Murphy Murph Cooper, Joseph's daughter, becomes a NASA scientist working under Professor Brand.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Donald\",\n",
      "            \"aliases\": [\n",
      "                \"Donald Cooper\"\n",
      "            ],\n",
      "            \"fragments\": [\n",
      "                \"Donald, Cooper's elderly father-in-law, supports the family as farmers.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Professor John Brand\",\n",
      "            \"aliases\": [\n",
      "                \"Professor Brand\"\n",
      "            ],\n",
      "            \"fragments\": [\n",
      "                \"Professor John Brand, a high-ranking NASA scientist and father of Amelia, is the director of the Lazarus and Endurance missions.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Tom Cooper\",\n",
      "            \"aliases\": [\n",
      "                \"Tom\"\n",
      "            ],\n",
      "            \"fragments\": [\n",
      "                \"Tom Cooper, Joseph's son, eventually takes charge of his father's farm.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"Doyle\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"Doyle, a high-ranking NASA member and Endurance crew member, is part of the team that travels through the wormhole.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"character\": \"CASE\",\n",
      "            \"aliases\": [],\n",
      "            \"fragments\": [\n",
      "                \"CASE is another intelligent robot assigned to assist the crew of the Endurance.\"\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Trial to achieve deduping\n",
    "#  \n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Sample JSON data as a string with a title for context\n",
    "title = \"Interstellar\"\n",
    "json_input = '''\n",
    "[\n",
    "    \n",
    "    {\n",
    "        \"character\": \"Dr. Mann\",\n",
    "        \"aliases\": [\n",
    "            \"Mann\"\n",
    "        ],\n",
    "        \"fragment\": [\n",
    "            \"Dr. Mann, a NASA astronaut, is sent to an icy planet during the Lazarus program.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"character\": \"Mann\",\n",
    "        \"aliases\": [],\n",
    "        \"fragment\": [\n",
    "            \"Mann was in the hibernation state when cooper arrives.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"character\": \"Murphy Cooper\",\n",
    "        \"aliases\": [],\n",
    "        \"fragment\": [\n",
    "            \"Today is my birthday and it's a special one because you once told me that when you came back we might be the same age.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"character\": \"Cooper\",\n",
    "        \"aliases\": [\n",
    "            \"tom cooper\",\n",
    "            \"cooper\"\n",
    "            \n",
    "        ],\n",
    "        \"fragment\": [\n",
    "            \"Referred to only as Cooper or Coop in the film.Cooper proudly identifies himself as an engineer as well as an astronaut and farmer but he has the soul of a goofball poet.Cooper's farewell to his daughter Murph, who's played by McKenzie Foy as a young girl, is shot very close-in and lit in warm cradling tones.We've always defined ourselves by the ability to overcome the impossible.Do not go gentle into that good night.Today is my birthday and it's a special one because you once told me that when you came back we might be the same age.Love is the one thing that transcends time and space.TARS talks plenty for both of us.Everybody good? Plenty of slaves for my robot colony.Joseph Cooper, a former NASA test pilot, reluctantly becomes a farmer after the agency was closed by the government.Dr. Amelia Brand, professor Brand's daughter and NASA scientist, is responsible for conducting planet colonization.Murphy Murph Cooper, Joseph's daughter, becomes a NASA scientist working under Professor Brand.Donald, Cooper's elderly father-in-law, supports the family as farmers.Tom Cooper, Joseph's son, eventually takes charge of his father's farm.Doyle, a high-ranking NASA member and Endurance crew member, is part of the team that travels through the wormhole.TARS is an intelligent robot assigned to assist the crew of the Endurance.CASE is another intelligent robot assigned to assist the crew of the Endurance.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"character\": \"Joseph Cooper\",\n",
    "        \"aliases\": [\n",
    "            \"Cooper\"\n",
    "        ],\n",
    "        \"fragment\": \"Joseph Cooper, a former NASA test pilot, reluctantly becomes a farmer after the agency was closed by the government.\"\n",
    "    }\n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "# Creating a chat completion request\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant skilled in understanding complex character relationships and providing guidance on merging similar characters into single entries. Given a JSON object list of characters, their aliases, and descriptions(fragments), identify duplicates and suggest how their data might be merged. The aim is to reduce duplication and create a cohesive character profile for each unique individual. Output the deduped JSON Object. Make semantic deduping.\"\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"The title of the show is '{title}'. Here is the character data: {json_input}. For now return a json onject with removed duplication, also concatinate there fragments if found similar character\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-16k\"\n",
    ")\n",
    "\n",
    "# Print the model's response\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e32ffb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"characters\": [\n",
      "    {\n",
      "      \"character\": \"Matthew McConaughey\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Matthew McConaughey stars in Interstellar as the main protagonist. The teaser trailer for Interstellar debuted December 13, 2013 and featured clips related to space exploration accompanied by a voiceover by Matthew McConaughey's character Cooper.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Anne Hathaway\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Anne Hathaway joins the cast of Interstellar alongside Matthew McConaughey. When Murph grows up into Jessica Chastain, a key member of Caine's NASA crew and a surrogate for the daughter that the elder Brand lost to the Endurance's mission.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Jessica Chastain\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Jessica Chastain appears in Interstellar as one of the main characters.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Christopher Nolan\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Christopher Nolan directs Interstellar, bringing his unique vision to the film. I think really space exploration to me has always represented the most hopeful and optimistic endeavor that mankind has ever really engaged with.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Matt Damon\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Matt Damon joins the cast of Interstellar in a supporting role.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Hoyte van Hoytema\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Hoyte van Hoytema serves as the director of photography for Interstellar.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Wally Pfister\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Wally Pfister, known for his work with Christopher Nolan, is replaced by Hoyte van Hoytema as the director of photography for Interstellar.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Paul Franklin\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Paul Franklin serves as the visual effects supervisor for Interstellar.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Kip Thorne\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Kip Thorne, a renowned physicist, serves as a science advisor for Interstellar. Thorne collaborated with Franklin and a team of 30 people at Double Negative.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Hans Zimmer\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Hans Zimmer composes the score for Interstellar, including the iconic theme.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Cooper\",\n",
      "      \"aliases\": [\n",
      "        \"Coop\"\n",
      "      ],\n",
      "      \"fragment\": \"Referred to only as Cooper or Coop in the film. We've always defined ourselves by the ability to overcome the impossible. Cooper proudly identifies himself as an engineer as well as an astronaut and farmer but he has the soul of a goofball poet.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Mann\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Mann was in hibernation when cooper arrived. Dr. Mann, a NASA astronaut, is sent to an icy planet during the Lazarus program.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Thorne\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Thorne collaborated with Franklin and a team of 30 people at Double Negative.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Nolan\",\n",
      "      \"aliases\": [\n",
      "        \"Christopher Nolan\"\n",
      "      ],\n",
      "      \"fragment\": \"Christopher Nolan and McConaughey made their first appearances at San Diego Comic-Con in July 2014 to promote Interstellar.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Professor Brand\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Do not go gentle into that good night. Professor John Brand, a high-ranking NASA scientist and father of Amelia, is the director of the Lazarus and Endurance missions.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Murph\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Cooper's farewell to his daughter Murph, who's played by McKenzie Foy as a young girl, is shot very close-in and lit in warm cradling tones.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"TARS\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Snappy comic patter mostly between Cooper and the ship's robot TARS, designed in Minecraft-style pixelish boxes and voiced by Bill Irwin. TARS is an intelligent robot assigned to assist the crew of the Endurance.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Professor John Brand\",\n",
      "      \"aliases\": [\n",
      "        \"Professor Brand\"\n",
      "      ],\n",
      "      \"fragment\": \"Professor John Brand, a high-ranking NASA scientist and father of Amelia, is the director of the Lazarus and Endurance missions.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Tom Cooper\",\n",
      "      \"aliases\": [\n",
      "        \"Tom\"\n",
      "      ],\n",
      "      \"fragment\": \"Tom Cooper, Joseph's son, eventually takes charge of his father's farm.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Donald\",\n",
      "      \"aliases\": [\n",
      "        \"Donald Cooper\"\n",
      "      ],\n",
      "      \"fragment\": \"Donald, Cooper's elderly father-in-law, supports the family as farmers.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Professor John Brand\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Professor John Brand, a high-ranking NASA scientist and father of Amelia, is the director of the Lazarus and Endurance missions.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Doyle\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Doyle, a high-ranking NASA member and Endurance crew member, is part of the team that travels through the wormhole.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"CASE\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"CASE is another intelligent robot assigned to assist the crew of the Endurance.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Extracted JSON Array String:\n",
      " [\n",
      "    {\n",
      "      \"character\": \"Matthew McConaughey\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Matthew McConaughey stars in Interstellar as the main protagonist. The teaser trailer for Interstellar debuted December 13, 2013 and featured clips related to space exploration accompanied by a voiceover by Matthew McConaughey's character Cooper.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Anne Hathaway\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Anne Hathaway joins the cast of Interstellar alongside Matthew McConaughey. When Murph grows up into Jessica Chastain, a key member of Caine's NASA crew and a surrogate for the daughter that the elder Brand lost to the Endurance's mission.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Jessica Chastain\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Jessica Chastain appears in Interstellar as one of the main characters.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Christopher Nolan\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Christopher Nolan directs Interstellar, bringing his unique vision to the film. I think really space exploration to me has always represented the most hopeful and optimistic endeavor that mankind has ever really engaged with.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Matt Damon\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Matt Damon joins the cast of Interstellar in a supporting role.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Hoyte van Hoytema\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Hoyte van Hoytema serves as the director of photography for Interstellar.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Wally Pfister\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Wally Pfister, known for his work with Christopher Nolan, is replaced by Hoyte van Hoytema as the director of photography for Interstellar.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Paul Franklin\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Paul Franklin serves as the visual effects supervisor for Interstellar.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Kip Thorne\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Kip Thorne, a renowned physicist, serves as a science advisor for Interstellar. Thorne collaborated with Franklin and a team of 30 people at Double Negative.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Hans Zimmer\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Hans Zimmer composes the score for Interstellar, including the iconic theme.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Cooper\",\n",
      "      \"aliases\": [\n",
      "        \"Coop\"\n",
      "      ],\n",
      "      \"fragment\": \"Referred to only as Cooper or Coop in the film. We've always defined ourselves by the ability to overcome the impossible. Cooper proudly identifies himself as an engineer as well as an astronaut and farmer but he has the soul of a goofball poet.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Mann\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Mann was in hibernation when cooper arrived. Dr. Mann, a NASA astronaut, is sent to an icy planet during the Lazarus program.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Thorne\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Thorne collaborated with Franklin and a team of 30 people at Double Negative.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Nolan\",\n",
      "      \"aliases\": [\n",
      "        \"Christopher Nolan\"\n",
      "      ],\n",
      "      \"fragment\": \"Christopher Nolan and McConaughey made their first appearances at San Diego Comic-Con in July 2014 to promote Interstellar.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Professor Brand\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Do not go gentle into that good night. Professor John Brand, a high-ranking NASA scientist and father of Amelia, is the director of the Lazarus and Endurance missions.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Murph\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Cooper's farewell to his daughter Murph, who's played by McKenzie Foy as a young girl, is shot very close-in and lit in warm cradling tones.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"TARS\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Snappy comic patter mostly between Cooper and the ship's robot TARS, designed in Minecraft-style pixelish boxes and voiced by Bill Irwin. TARS is an intelligent robot assigned to assist the crew of the Endurance.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Professor John Brand\",\n",
      "      \"aliases\": [\n",
      "        \"Professor Brand\"\n",
      "      ],\n",
      "      \"fragment\": \"Professor John Brand, a high-ranking NASA scientist and father of Amelia, is the director of the Lazarus and Endurance missions.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Tom Cooper\",\n",
      "      \"aliases\": [\n",
      "        \"Tom\"\n",
      "      ],\n",
      "      \"fragment\": \"Tom Cooper, Joseph's son, eventually takes charge of his father's farm.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Donald\",\n",
      "      \"aliases\": [\n",
      "        \"Donald Cooper\"\n",
      "      ],\n",
      "      \"fragment\": \"Donald, Cooper's elderly father-in-law, supports the family as farmers.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Professor John Brand\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Professor John Brand, a high-ranking NASA scientist and father of Amelia, is the director of the Lazarus and Endurance missions.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"Doyle\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"Doyle, a high-ranking NASA member and Endurance crew member, is part of the team that travels through the wormhole.\"\n",
      "    },\n",
      "    {\n",
      "      \"character\": \"CASE\",\n",
      "      \"aliases\": [],\n",
      "      \"fragment\": \"CASE is another intelligent robot assigned to assist the crew of the Endurance.\"\n",
      "    }\n",
      "  ]\n",
      "Parsed Data:\n",
      " [{'character': 'Matthew McConaughey', 'aliases': [], 'fragment': \"Matthew McConaughey stars in Interstellar as the main protagonist. The teaser trailer for Interstellar debuted December 13, 2013 and featured clips related to space exploration accompanied by a voiceover by Matthew McConaughey's character Cooper.\"}, {'character': 'Anne Hathaway', 'aliases': [], 'fragment': \"Anne Hathaway joins the cast of Interstellar alongside Matthew McConaughey. When Murph grows up into Jessica Chastain, a key member of Caine's NASA crew and a surrogate for the daughter that the elder Brand lost to the Endurance's mission.\"}, {'character': 'Jessica Chastain', 'aliases': [], 'fragment': 'Jessica Chastain appears in Interstellar as one of the main characters.'}, {'character': 'Christopher Nolan', 'aliases': [], 'fragment': 'Christopher Nolan directs Interstellar, bringing his unique vision to the film. I think really space exploration to me has always represented the most hopeful and optimistic endeavor that mankind has ever really engaged with.'}, {'character': 'Matt Damon', 'aliases': [], 'fragment': 'Matt Damon joins the cast of Interstellar in a supporting role.'}, {'character': 'Hoyte van Hoytema', 'aliases': [], 'fragment': 'Hoyte van Hoytema serves as the director of photography for Interstellar.'}, {'character': 'Wally Pfister', 'aliases': [], 'fragment': 'Wally Pfister, known for his work with Christopher Nolan, is replaced by Hoyte van Hoytema as the director of photography for Interstellar.'}, {'character': 'Paul Franklin', 'aliases': [], 'fragment': 'Paul Franklin serves as the visual effects supervisor for Interstellar.'}, {'character': 'Kip Thorne', 'aliases': [], 'fragment': 'Kip Thorne, a renowned physicist, serves as a science advisor for Interstellar. Thorne collaborated with Franklin and a team of 30 people at Double Negative.'}, {'character': 'Hans Zimmer', 'aliases': [], 'fragment': 'Hans Zimmer composes the score for Interstellar, including the iconic theme.'}, {'character': 'Cooper', 'aliases': ['Coop'], 'fragment': \"Referred to only as Cooper or Coop in the film. We've always defined ourselves by the ability to overcome the impossible. Cooper proudly identifies himself as an engineer as well as an astronaut and farmer but he has the soul of a goofball poet.\"}, {'character': 'Mann', 'aliases': [], 'fragment': 'Mann was in hibernation when cooper arrived. Dr. Mann, a NASA astronaut, is sent to an icy planet during the Lazarus program.'}, {'character': 'Thorne', 'aliases': [], 'fragment': 'Thorne collaborated with Franklin and a team of 30 people at Double Negative.'}, {'character': 'Nolan', 'aliases': ['Christopher Nolan'], 'fragment': 'Christopher Nolan and McConaughey made their first appearances at San Diego Comic-Con in July 2014 to promote Interstellar.'}, {'character': 'Professor Brand', 'aliases': [], 'fragment': 'Do not go gentle into that good night. Professor John Brand, a high-ranking NASA scientist and father of Amelia, is the director of the Lazarus and Endurance missions.'}, {'character': 'Murph', 'aliases': [], 'fragment': \"Cooper's farewell to his daughter Murph, who's played by McKenzie Foy as a young girl, is shot very close-in and lit in warm cradling tones.\"}, {'character': 'TARS', 'aliases': [], 'fragment': \"Snappy comic patter mostly between Cooper and the ship's robot TARS, designed in Minecraft-style pixelish boxes and voiced by Bill Irwin. TARS is an intelligent robot assigned to assist the crew of the Endurance.\"}, {'character': 'Professor John Brand', 'aliases': ['Professor Brand'], 'fragment': 'Professor John Brand, a high-ranking NASA scientist and father of Amelia, is the director of the Lazarus and Endurance missions.'}, {'character': 'Tom Cooper', 'aliases': ['Tom'], 'fragment': \"Tom Cooper, Joseph's son, eventually takes charge of his father's farm.\"}, {'character': 'Donald', 'aliases': ['Donald Cooper'], 'fragment': \"Donald, Cooper's elderly father-in-law, supports the family as farmers.\"}, {'character': 'Professor John Brand', 'aliases': [], 'fragment': 'Professor John Brand, a high-ranking NASA scientist and father of Amelia, is the director of the Lazarus and Endurance missions.'}, {'character': 'Doyle', 'aliases': [], 'fragment': 'Doyle, a high-ranking NASA member and Endurance crew member, is part of the team that travels through the wormhole.'}, {'character': 'CASE', 'aliases': [], 'fragment': 'CASE is another intelligent robot assigned to assist the crew of the Endurance.'}]\n",
      "Processed JSON has been saved to data/Deduped_Entity_json/Interstellar/Interstellar_Desuped_llm3.json\n"
     ]
    }
   ],
   "source": [
    "# Latest deduping usinf llm SEMANTIC DEDUPING working code\n",
    "from openai import OpenAI\n",
    "import re\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # print(data)\n",
    "    return data\n",
    "\n",
    "# Function to write to a JSON file\n",
    "def write_json(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def process_json_with_openai(title, json_input):\n",
    "    # completion = client.chat.completions.create(\n",
    "\n",
    "    #     messages=[\n",
    "    #         {\n",
    "    #             \"role\": \"system\",\n",
    "    #             \"content\": \"You are a helpful assistant skilled in understanding complex character relationships and providing guidance on merging similar characters into single entries. Given a JSON object list of characters, their aliases, and descriptions(fragments), identify duplicates and suggest how their data might be merged. The aim is to reduce duplication and create a cohesive character profile for each unique individual. Output the deduped JSON Object. Make semantic deduping.\"\n",
    "    #         },\n",
    "\n",
    "    #         {\n",
    "    #             \"role\": \"user\",\n",
    "    #             \"content\": f\"The title of the show is '{title}'. Here is the character data: {json_input}. For now return a json onject with removed duplication, also concatinate there fragments and aliases if found sematically same\",\n",
    "    #         }\n",
    "    #     ],\n",
    "    #     model=\"gpt-3.5-turbo-16k\"\n",
    "    # )\n",
    "    completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant skilled in understanding complex character relationships and providing guidance on merging similar characters into single entries. Given a JSON object list of characters, their aliases, and descriptions(fragments), identify duplicates and suggest how their data might be merged. The aim is to reduce duplication and create a cohesive character profile for each unique individual. Output the deduped JSON Object. Make semantic deduping.\"\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"The title of the show is '{title}'. Here is the character data: {json_input}. For now return a json onject with removed duplication, also concatinate there fragments if found similar character\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-16k\"\n",
    "    )\n",
    "    res=completion.choices[0].message.content\n",
    "    # print(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_file_path = 'data/Merged_Entity_json/Interstellar (film)/Interstellar (film)_Merged.json'  # Path to your input JSON file\n",
    "output_file_path = 'data/Deduped_Entity_json/Interstellar/Interstellar_Desuped_llm3.json'  # Path to save the output JSON file\n",
    "\n",
    "# Read the input JSON file\n",
    "input_data = read_json(input_file_path)\n",
    "title = \"Interstellar\"\n",
    "# Process the JSON with OpenAI\n",
    "output_data = process_json_with_openai(title,(input_data))\n",
    "print(output_data)\n",
    "\n",
    "json_array_match = re.search(r'\\[\\s*\\{[\\s\\S]*?\\}\\s*\\]', output_data)\n",
    "if json_array_match:\n",
    "    json_array_str = json_array_match.group(0)\n",
    "    print(\"Extracted JSON Array String:\\n\", json_array_str)\n",
    "else:\n",
    "    print(\"No JSON array found in the response\")\n",
    "    json_array_str = '[]'  # Fallback to an empty list if no JSON is found\n",
    "\n",
    "# Parse the extracted JSON array\n",
    "try:\n",
    "    parsed_data = json.loads(json_array_str)\n",
    "    print(\"Parsed Data:\\n\", parsed_data)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSONDecodeError: {e}\")\n",
    "    parsed_data = []\n",
    "\n",
    "\n",
    "# Write the output JSON file\n",
    "write_json(output_file_path, parsed_data)\n",
    "\n",
    "print(f\"Processed JSON has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb6015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ebe2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.11/site-packages (1.32.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (2.5.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.51.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a57a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
